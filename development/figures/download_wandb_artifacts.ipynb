{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download artifacts/data from wandb for figure generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRuns(project, sweepid, endpoints):\n",
    "\n",
    "    api = wandb.Api()\n",
    "\n",
    "    # Project is specified by <entity/project-name>\n",
    "    runs = api.sweep(project+'/'+sweepid).runs\n",
    "\n",
    "    summary_list, config_list, name_list, ids = [], [], [], []\n",
    "    for run in runs: \n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files\n",
    "\n",
    "        if run.config['endpoints'] == endpoints:\n",
    "\n",
    "            summary_list.append(run.summary._json_dict)\n",
    "\n",
    "            # .config contains the hyperparameters.\n",
    "            #  We remove special values that start with _.\n",
    "            config_list.append(\n",
    "                {k: v for k,v in run.config.items()\n",
    "                if not k.startswith('_')})\n",
    "\n",
    "            # .name is the human-readable name of the run.\n",
    "            name_list.append(run.name)\n",
    "            ids.append(run.id)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"summary\": summary_list,\n",
    "        \"config\": config_list,\n",
    "        \"name\": name_list,\n",
    "        \"run_ids\": ids,\n",
    "        'sweepid': sweepid\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadArtifact(wandbrun, runid, entity, project, artifact_name, fname, version, type):\n",
    "    artifact = wandbrun.use_artifact(f'{entity}/{project}/run-{runid}-{artifact_name}:{version}', type=type)\n",
    "    artifact_dir = artifact.download()\n",
    "    jsonfile = json.load(open(f'{artifact_dir}/{fname}.table.json'))\n",
    "\n",
    "    cols = jsonfile['columns']\n",
    "    data = jsonfile['data']\n",
    "    df = pd.DataFrame(data=data, columns=cols)\n",
    "\n",
    "    columns=['internal_id', 'Conc_sign', 'species_group', 'Pubchem_CID', 'xlogp', 'mw', 'Canonical_SMILES','Lineage','OneHotEnc_effect','OneHotEnc_endpoint']\n",
    "    \n",
    "    for col in columns:\n",
    "        try:\n",
    "            df.drop(columns=col, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineKFoldPredictions(wandbrun, runs_df, project, version, artifact_name, name):\n",
    "\n",
    "    df = LoadArtifact(\n",
    "            wandbrun = wandbrun, \n",
    "            runid= runs_df.run_ids[0], \n",
    "            entity = 'ecotoxformer', \n",
    "            project = project, \n",
    "            artifact_name = artifact_name, \n",
    "            fname = name,\n",
    "            version = version,\n",
    "            type = 'run_table')\n",
    "    \n",
    "    df[['seed', 'fold_id']] = [runs_df.config[0]['seed'], runs_df.config[0]['fold_id']]\n",
    "\n",
    "    for i in tqdm(range(1,len(runs_df),1)):\n",
    "        df2 = LoadArtifact(\n",
    "            wandbrun = wandbrun, \n",
    "            runid= runs_df.run_ids[i], \n",
    "            entity = 'ecotoxformer', \n",
    "            project = project, \n",
    "            artifact_name = artifact_name, \n",
    "            fname = name,\n",
    "            version = version,\n",
    "            type = 'run_table')\n",
    "\n",
    "        df2[['seed', 'fold_id']] = [runs_df.config[i]['seed'], runs_df.config[i]['fold_id']]\n",
    "\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def CombinePredictions(wandbrun, runs_df, project, version, artifact_name, name):\n",
    "\n",
    "    df = LoadArtifact(\n",
    "            wandbrun = wandbrun, \n",
    "            runid= runs_df.run_ids[0], \n",
    "            entity = 'ecotoxformer', \n",
    "            project = project, \n",
    "            artifact_name = artifact_name, \n",
    "            fname = name,\n",
    "            version = version,\n",
    "            type = 'run_table')\n",
    "    df[['base_model', 'loss_fun']] = [runs_df['base_model'][0], runs_df['loss_fun'][0]]\n",
    "    for i in tqdm(range(1,len(runs_df),1)):\n",
    "        df2 = LoadArtifact(\n",
    "            wandbrun = wandbrun, \n",
    "            runid= runs_df.run_ids[i], \n",
    "            entity = 'ecotoxformer', \n",
    "            project = project, \n",
    "            artifact_name = artifact_name, \n",
    "            fname = name,\n",
    "            version = version,\n",
    "            type = 'run_table')\n",
    "        df2[['base_model', 'loss_fun']] = [runs_df['base_model'][i], runs_df['loss_fun'][i]]\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandbrun = wandb.init(project=\"artifacts-analysis\", job_type='tmp_run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'Judson'\n",
    "SWEEP_ID = 'jnkzjb20'\n",
    "ENDPOINT = ['EC50']\n",
    "ARTIFACT_NAME = 'BestValidationResults'\n",
    "DOWNLOADED_ARTIFACT_NAME = 'Best Validation Results' #Split ARTIFACT_NAME by capital letters\n",
    "ARTIFACT_VERSION = 'v0'\n",
    "FILENAME = '' #Filename to which results will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = GetRuns(project=f\"ecotoxformer/{PROJECT_NAME}/\", sweepid=SWEEP_ID, endpoints=ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43d9f9c8595472ea0cf86dc9d392074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concatenated_results = CombineKFoldPredictions(\n",
    "    wandbrun=wandbrun, \n",
    "    runs_df=runs_df,\n",
    "    project=PROJECT_NAME, \n",
    "    version=ARTIFACT_VERSION, \n",
    "    artifact_name=ARTIFACT_NAME,\n",
    "    name=DOWNLOADED_ARTIFACT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_results = pd.DataFrame()\n",
    "runs_df = pd.DataFrame(['56vivdmi','dl8u33ui','78lq56v1','128aaus7'], columns=['run_ids'])\n",
    "runs_df['loss_fun'] = ['L1Loss','L1Loss','MSELoss','MSELoss']\n",
    "runs_df['base_model'] = ['seyonec/SMILES_tokenized_PubChem_shard00_160k','seyonec/PubChem10M_SMILES_BPE_450k','seyonec/SMILES_tokenized_PubChem_shard00_160k','seyonec/PubChem10M_SMILES_BPE_450k']\n",
    "for i in tqdm(range(5)):\n",
    "    df = CombinePredictions(wandbrun, runs_df, 'base_model_sweep_RDKit', 'v0', f'BestValidationResults{i+1}', f'Best Validation Results {i+1}')\n",
    "    df['fold_id'] = i+1\n",
    "    concatenated_results = pd.concat([concatenated_results, df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_results.to_csv(f'../../data/results/{FILENAME}.csv', index=False, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_results.to_pickle(f'../../data/results/{FILENAME}.pkl', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
