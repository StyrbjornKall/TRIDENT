{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and launch parameter and hyperparameter SWEEP on multiple GPUs\n",
    "This script performes 5-fold cross-validation (hyper)parameter sweeps used to run the following sweeps:\n",
    "1. ChemBERTa version and loss function\n",
    "    - variable parameters: [learning rate, loss function, ChemBERTa version]\n",
    "    - config file: `BPEtok10M.yaml` and `SMILEStok1M.yaml`\n",
    "2. Batch size\n",
    "    - variable parameters: [learning rate, batch size]\n",
    "    - config file: `batch_size.yaml`\n",
    "3. EC50 Model configuration\n",
    "    - variable parameters: [learning rate, number of reinitialized encoders, number of hidden layers, hidden layer sizes]\n",
    "    - config file: `2_hidden_layers_hpsweep.yaml`, `3_hidden_layers_hpsweep.yaml` and `4_hidden_layers_hpsweep.yaml`\n",
    "4. EC10 Model configuration\n",
    "    - variable parameters: [learning rate, number of reinitialized encoders, number of hidden layers, hidden layer sizes]\n",
    "    - config file: `2_hidden_layers_hpsweep.yaml`, `3_hidden_layers_hpsweep.yaml` and `4_hidden_layers_hpsweep.yaml`\n",
    "5. EC50 and EC10 combo Model configuration\n",
    "    - variable parameters: [learning rate, number of reinitialized encoders, number of hidden layers, hidden layer sizes]\n",
    "    - config file: `2_hidden_layers_hpsweep.yaml`, `3_hidden_layers_hpsweep.yaml` and `4_hidden_layers_hpsweep.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T12:20:06.573146Z",
     "iopub.status.busy": "2022-04-07T12:20:06.572791Z",
     "iopub.status.idle": "2022-04-07T12:20:06.594684Z",
     "shell.execute_reply": "2022-04-07T12:20:06.594057Z",
     "shell.execute_reply.started": "2022-04-07T12:20:06.573114Z"
    },
    "id": "MjtASeFVRMCW",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from ..utils.Get_data_for_model import *\n",
    "from ..utils.Build_data_for_pytorch import *\n",
    "from ..utils.Build_model_pytorch import *\n",
    "from ..utils.Inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.327439Z",
     "iopub.status.busy": "2022-04-01T05:46:31.326981Z",
     "iopub.status.idle": "2022-04-01T05:46:31.394170Z",
     "shell.execute_reply": "2022-04-01T05:46:31.393370Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.327400Z"
    },
    "id": "d60ceda8",
    "outputId": "93f57e48-3fb9-49f9-f4b0-48d51e958abd",
    "papermill": {
     "duration": 0.087271,
     "end_time": "2022-03-04T15:15:20.579086",
     "exception": false,
     "start_time": "2022-03-04T15:15:20.491815",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs on node: NVIDIA A100-SXM4-40GB\n",
      "Number of GPUs available: 1\n",
      "Using cuda:0 device\n",
      "42.35 Gb free on CUDA\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GPUinfo(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ENTITYNAME = 'ecotoxformer'\n",
    "PROJECTNAME = 'KFold_HP_sweep_EC50EC10_RDKit'\n",
    "SWEEPID = 'b9s38isu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.395755Z",
     "iopub.status.busy": "2022-04-01T05:46:31.395495Z",
     "iopub.status.idle": "2022-04-01T05:46:37.015133Z",
     "shell.execute_reply": "2022-04-01T05:46:37.014459Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.395718Z"
    },
    "id": "826933f3",
    "outputId": "3657c92a-871e-4cd8-d2c6-c1b913230444",
    "papermill": {
     "duration": 1.576782,
     "end_time": "2022-03-04T15:15:22.19011",
     "exception": true,
     "start_time": "2022-03-04T15:15:20.613328",
     "status": "failed"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T12:20:22.950455Z",
     "iopub.status.busy": "2022-04-07T12:20:22.950204Z",
     "iopub.status.idle": "2022-04-07T12:20:25.431843Z",
     "shell.execute_reply": "2022-04-07T12:20:25.431174Z",
     "shell.execute_reply.started": "2022-04-07T12:20:22.950420Z"
    },
    "id": "tZnEXOn6RMCi",
    "outputId": "398ce7c0-98b7-4af9-ad2e-0b165ec19634",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def GetData(data, config, dir):\n",
    "    processor = PreprocessData(dataframe=data)\n",
    "\n",
    "    processor.FilterData(\n",
    "        concentration_thresh=config.conc_thresh,\n",
    "        endpoint=config.endpoints,\n",
    "        effect=config.effects,\n",
    "        species_groups=config.species_groups,\n",
    "        log_data=True,\n",
    "        concentration_sign=config.concentration_sign)\n",
    "\n",
    "    processor.GetPubchemCID(dir+'dict_of_SMILES_and_CID')\n",
    "    processor.GetMetadata(dir+'MSc_Pubchem_metadata.csv', ['cmpdname'])\n",
    "    processor.GetCanonicalSMILES()\n",
    "    processor.GetOneHotEndpoint(config.endpoints)\n",
    "    processor.GetOneHotEffect(config.effects)\n",
    "    processor.ConcatenateOneHotEnc()\n",
    "\n",
    "    data = processor.dataframe\n",
    "    fc1 = len(data.OneHotEnc_concatenated.iloc[0])\n",
    "    \n",
    "    return data, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    torch.manual_seed(seed) # pytorch random seed\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def GetLayers(config):\n",
    "    if config.n_hidden_layers == 1:\n",
    "        return [config.layer_1]\n",
    "    elif config.n_hidden_layers == 2:\n",
    "        return [config.layer_1, config.layer_2]\n",
    "    elif config.n_hidden_layers == 3:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3]\n",
    "    elif config.n_hidden_layers == 4:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3, config.layer_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbr1fJlDRMCq"
   },
   "source": [
    "### Train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def RunTrainingEpochs(data, folds, fold_id, config, fc1, global_step):\n",
    "    \n",
    "    chemberta = AutoModel.from_pretrained(config.base_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "    DataLoaders = BuildDataLoader_KFold(\n",
    "                                    df = data,\n",
    "                                    folds = folds,\n",
    "                                    fold_id=fold_id, \n",
    "                                    variables = config.inputs,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed, \n",
    "                                    tokenizer = tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoaders.BuildTrainingLoader(sampler_choice=config.sampling_procedure, num_workers=2, weight_args=[config.inputs[0],'COMBINED_endpoint','COMBINED_effect'])\n",
    "    val_dataloader = DataLoaders.BuildValidationLoader(sampler_choice='SequentialSampler', num_workers=2) \n",
    "    print('Successfully built dataloader')\n",
    "\n",
    "######## MODEL ##################################################################################\n",
    "    dnn_module = DNN_module(\n",
    "                        one_hot_enc_len=fc1,\n",
    "                        n_hidden_layers=config.n_hidden_layers,\n",
    "                        layer_sizes=GetLayers(config),\n",
    "                        dropout=config.dropout,\n",
    "                        activation='ReLU')\n",
    "\n",
    "    model = fishbAIT(roberta=chemberta, dnn=dnn_module)\n",
    "\n",
    "    model = Modify_architecture(model).FreezeModel(model, config.n_frozen_layers, config.freeze_embedding)\n",
    "    model = Modify_architecture(model).ReinitializeEncoderLayers(model, reinit_n_layers=config.reinit_n_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "######## TRAINING CONFIG ##################################################################################   \n",
    "    model_parameters = Modify_architecture(model).LLRD(model, init_lr = config.lr)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model_parameters, lr=config.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*config.epochs*len(train_dataloader), num_training_steps=config.epochs*len(train_dataloader))\n",
    "    print('Successfully built optimizer')\n",
    "\n",
    "    if config.loss_fun == 'MSELoss':\n",
    "        loss_fun = nn.MSELoss()\n",
    "    else:\n",
    "        loss_fun = nn.L1Loss()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_loss_norm = np.inf\n",
    "    \n",
    "    batch_num = [0,0]\n",
    "    \n",
    "######## RUN TRAINING ##################################################################################\n",
    "\n",
    "    avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, -1, global_step-1)\n",
    "\n",
    "    if median_loss < best_val_loss:\n",
    "        best_val_loss = median_loss\n",
    "    \n",
    "    Best_Validation_Median_Loss = []\n",
    "    Best_Validation_Median_Loss_Normalized = []\n",
    "    Best_Validation_Mean_Loss_Normalized = []\n",
    "    print(\"\\nRunning epochs...\")\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "\n",
    "        avg_loss, median_loss, total_preds, total_labels, batch_num = train(config, model, train_dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        if median_loss_norm < best_val_loss_norm:\n",
    "            best_val_loss = median_loss\n",
    "            best_val_loss_norm = median_loss_norm\n",
    "            best_validation_results = val_results\n",
    "            best_validation_mean_norm_loss = avg_loss_norm\n",
    "\n",
    "        wandb.log({'Best Validation Median Loss': best_val_loss,\n",
    "                    'Best Validation Median Loss Normalized': best_val_loss_norm,\n",
    "                    'Best Validation Mean Loss Normalized': best_validation_mean_norm_loss,\n",
    "                    'global_step': global_step})\n",
    "        \n",
    "        Best_Validation_Median_Loss.append(best_val_loss)\n",
    "        Best_Validation_Median_Loss_Normalized.append(best_val_loss_norm)\n",
    "        Best_Validation_Mean_Loss_Normalized.append(best_validation_mean_norm_loss)\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        \n",
    "    del model\n",
    "    del optimizer\n",
    "    del loss_fun\n",
    "    del chemberta\n",
    "    del tokenizer\n",
    "\n",
    "    return Best_Validation_Median_Loss, Best_Validation_Median_Loss_Normalized, Best_Validation_Mean_Loss_Normalized, global_step, best_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:58.430879Z",
     "iopub.status.busy": "2022-05-04T11:40:58.425461Z",
     "iopub.status.idle": "2022-05-04T11:40:59.477198Z",
     "shell.execute_reply": "2022-05-04T11:40:59.476376Z",
     "shell.execute_reply.started": "2022-05-04T11:40:58.430838Z"
    },
    "id": "4BFUym5kRMCq",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(args, model, dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step):\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    total_loss = 0\n",
    "    total_preds=[]\n",
    "    total_labels=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = [r.to(device) for r in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(sent_id, mask, duration, onehot)\n",
    "        loss = loss_fun(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        total_labels.append(labels)\n",
    "        \n",
    "        \n",
    "        wandb.log({\n",
    "            \"Training Batch Loss\": loss.item(),\n",
    "            \"Learning Rate\": optimizer.param_groups[0][\"lr\"], \n",
    "            'training batch': batch_num[0]\n",
    "        })\n",
    "        batch_num[0] += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    median_loss = np.median(abs(total_preds - total_labels))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training Loss function\": avg_loss,\n",
    "        \"Training Mean Loss\": np.mean(abs(total_preds - total_labels)), \n",
    "        'training epoch': epoch,\n",
    "        \"Training Median Loss\": np.median(abs(total_preds - total_labels)),\n",
    "        \"Training RMSE Loss\": np.sqrt(np.mean((total_labels-total_preds)**2)),\n",
    "        'global_step': global_step})\n",
    "    \n",
    "    return avg_loss, median_loss, total_preds, total_labels, batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:59.479019Z",
     "iopub.status.busy": "2022-05-04T11:40:59.478572Z",
     "iopub.status.idle": "2022-05-04T11:40:59.492216Z",
     "shell.execute_reply": "2022-05-04T11:40:59.491432Z",
     "shell.execute_reply.started": "2022-05-04T11:40:59.478936Z"
    },
    "id": "NPhmG5KCRMCq",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, dataloader, dataset, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    val_results = dataset.copy()\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = [t.to(device) for t in batch.values()]\n",
    "\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(sent_id, mask, duration, onehot)\n",
    "                loss = loss_fun(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "        batch_num[1] += 1\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    val_results['labels'] = total_labels\n",
    "    val_results['preds'] = total_preds\n",
    "    val_results['residuals'] = val_results.labels-val_results.preds\n",
    "    val_results['L1Error'] = abs(total_labels - total_preds)\n",
    "    median_loss = val_results.L1Error.median()\n",
    "    val_results_normalized = CalculateWeightedAverage(val_results)\n",
    "    median_loss_norm = abs(val_results_normalized.residuals).median()\n",
    "    avg_loss_norm = abs(val_results_normalized.residuals).mean()\n",
    "    wandb.log({\n",
    "        \"Validation Loss function\": avg_loss,\n",
    "        \"Validation Mean Loss\": val_results.L1Error.mean(),\n",
    "        \"Validation Median Loss\": median_loss,\n",
    "        \"Validation Loss Normalized\": median_loss_norm,\n",
    "        \"Validation Mean Loss Normalized\": avg_loss_norm,\n",
    "        \"Validation RMSE Loss Normalized\": np.sqrt(((val_results_normalized.labels - val_results_normalized.preds)**2).mean()),\n",
    "        'validation epoch': epoch,\n",
    "        'global_step': global_step\n",
    "        })\n",
    "        \n",
    "    return avg_loss, avg_loss_norm, median_loss, median_loss_norm, total_preds, batch_num, val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def trainer(config=None):\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    SetSeed(42)\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "        sweepconfig = wandb.config\n",
    "\n",
    "        datadir = '../data/'\n",
    "        utilsdir = '../utils/'\n",
    "\n",
    "    ######## DATA ##################################################################################\n",
    "        data = pd.read_excel(datadir+'Preprocessed_complete_data.xlsx', encoding='windows-1252')\n",
    "        data, fc1 = GetData(data, sweepconfig, utilsdir)\n",
    "        print('Successfully loaded data')\n",
    "\n",
    "        folds = Make_KFolds().Split(data[sweepconfig.inputs[0]], k_folds=sweepconfig.k_folds, seed=sweepconfig.seed)\n",
    "        print('Successfully built folds')\n",
    "        name = wandb.run.name\n",
    "        \n",
    "        Avg_Best_Validation_Median_Loss = np.zeros((sweepconfig.k_folds, sweepconfig.epochs))\n",
    "        Avg_Best_Validation_Median_Loss_Normalized = np.zeros((sweepconfig.k_folds, sweepconfig.epochs))\n",
    "        Avg_Best_Validation_Mean_Loss_Normalized = np.zeros((sweepconfig.k_folds, sweepconfig.epochs))\n",
    "        print(f'\\n Running {sweepconfig.k_folds} folds')\n",
    "        global_step = 0\n",
    "        for fold_id in tqdm(range(1, sweepconfig.k_folds+1, 1)):\n",
    "            Best_Validation_Median_Loss, Best_Validation_Median_Loss_Normalized, Best_Validation_Mean_Loss_Normalized, global_step, best_validation_results = RunTrainingEpochs(data, folds, fold_id, sweepconfig, fc1, global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if sweepconfig.save_results == True:\n",
    "                wandb.log({f\"Best Validation Results {fold_id}\": wandb.Table(dataframe=best_validation_results)})\n",
    "            Avg_Best_Validation_Median_Loss[fold_id-1,:] = Best_Validation_Median_Loss\n",
    "            Avg_Best_Validation_Median_Loss_Normalized[fold_id-1,:] = Best_Validation_Median_Loss_Normalized\n",
    "            Avg_Best_Validation_Mean_Loss_Normalized[fold_id-1,:] = Best_Validation_Mean_Loss_Normalized\n",
    "\n",
    "        Avg_Best_Validation_Median_Loss.mean(axis=0)\n",
    "        Avg_Best_Validation_Median_Loss_Normalized.mean(axis=0)\n",
    "\n",
    "        # Log the sweep metric as the average performance\n",
    "        for i in range(sweepconfig.epochs):\n",
    "            wandb.log({\n",
    "                'Avg Best Validation Median Loss Normalized': Avg_Best_Validation_Median_Loss_Normalized.mean(axis=0)[i],\n",
    "                'Avg Best Validation Median Loss Normalized std': np.std(Avg_Best_Validation_Median_Loss_Normalized, axis=0)[i],\n",
    "                'Avg Best Validation Median Loss': Avg_Best_Validation_Median_Loss.mean(axis=0)[i],\n",
    "                'Avg Best Validation Median Loss std': np.std(Avg_Best_Validation_Median_Loss, axis=0)[i],\n",
    "                'Avg Best Validation Mean Loss Normalized': Avg_Best_Validation_Mean_Loss_Normalized.mean(axis=0)[i],\n",
    "                'epoch': i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wandb.agent(f'{ENITITYNAME+'/'+PROJECTNAME+'/'+SWEEPID, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers2",
   "language": "python",
   "name": "transformers"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
