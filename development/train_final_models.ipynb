{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Training\n",
    "**Purpose:** This script is used to train the final models. The script may also be used to try out other model configurations if desired.\n",
    "\n",
    "**Dependency:** `hyperparameter_sweep.ipynb`, `Kfold_crossvalidation_sweep.ipynb`. The model configurations specified in this script are depermined by the model development sweeps (`hyperparameter_sweep.ipynb`). The number of epochs used when training the final models are determined by examining the 10x10 K-fold cross-validation runs (`Kfold_crossvalidation_sweep.ipynb`) and vary depending on model version due to overfitting being more or less prone to happen for the different datasets.\n",
    "\n",
    "**Consecutive scripts:** After running this script the following scripts may be executed. `push_model_to_huggingface.ipynb`, `download_wandb_artifacts.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjtASeFVRMCW"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/cephyr/users/skall/Alvis/TRIDENT/development/development_utils/')\n",
    "import os\n",
    "os.chdir('/cephyr/users/skall/Alvis/TRIDENT/development/')\n",
    "\n",
    "from development_utils.preprocessing.Get_data_for_model import PreprocessData\n",
    "from development_utils.training.Build_Pytorch_Dataset_and_DataLoader import BuildDataLoader_with_trainval_ratio\n",
    "from development_utils.training.Build_Pytorch_model import TRIDENT, DNN_module, GPUinfo, Modify_architecture\n",
    "from development_utils.training.PerformanceCalculations import CalculateWeightedAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJGBSg2BRMCX",
    "outputId": "f2658262-2eb7-4d1e-9edd-10140649b5c0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GPUinfo(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITYNAME = 'ecotoxformer'\n",
    "PROJECTNAME = 'Final_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.395755Z",
     "iopub.status.busy": "2022-04-01T05:46:31.395495Z",
     "iopub.status.idle": "2022-04-01T05:46:37.015133Z",
     "shell.execute_reply": "2022-04-01T05:46:37.014459Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.395718Z"
    },
    "id": "826933f3",
    "outputId": "3657c92a-871e-4cd8-d2c6-c1b913230444",
    "papermill": {
     "duration": 1.576782,
     "end_time": "2022-03-04T15:15:22.19011",
     "exception": true,
     "start_time": "2022-03-04T15:15:20.613328",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(entity=ENTITYNAME, project=PROJECTNAME, notes='', dir = '/mimer/NOBACKUP/groups/snic2022-22-552/skall/wandb/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PAY ATTENTION**\n",
    "The following parameters depend on results from other scripts and can assume different values depending on which model version shopuld be fine-tuned. \n",
    "\n",
    "The values used in the publication are the following:\n",
    "\n",
    "F-M50:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['fish']  |\n",
    "| endpoints         | ['EC50']  |\n",
    "| effects           | ['MOR']  |\n",
    "| epochs            | 35  |\n",
    "| lr                | 0.00015  |\n",
    "\n",
    "F-M10:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['fish']  |\n",
    "| endpoints         | ['EC10', 'NOEC']  |\n",
    "| effects           | ['MOR','DVP','ITX','REP','MPH','POP','GRO']  |\n",
    "| epochs            | 35  |\n",
    "| lr                | 0.0005  |\n",
    "\n",
    "F-M5010:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['fish']  |\n",
    "| endpoints         | ['EC50', 'EC10', 'NOEC']  |\n",
    "| effects           | ['MOR','DVP','ITX','REP','MPH','POP','GRO']  |\n",
    "| epochs            | 25  |\n",
    "| lr                | 0.0002  |\n",
    "\n",
    "A-M50:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['algae']  |\n",
    "| endpoints         | ['EC50']  |\n",
    "| effects           | ['POP']  |\n",
    "| epochs            | 25  |\n",
    "| lr                | 0.00015  |\n",
    "\n",
    "A-M10:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['algae']  |\n",
    "| endpoints         | ['EC10','NOEC']  |\n",
    "| effects           | ['POP']  |\n",
    "| epochs            | 30  |\n",
    "| lr                | 0.0005  |\n",
    "\n",
    "A-M5010:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['algae']  |\n",
    "| endpoints         | ['EC50','EC10','NOEC']  |\n",
    "| effects           | ['POP']  |\n",
    "| epochs            | 35  |\n",
    "| lr                | 0.0002  |\n",
    "\n",
    "I-M50:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['crustaceans']  |\n",
    "| endpoints         | ['EC50']  |\n",
    "| effects           | ['MOR','ITX']  |\n",
    "| epochs            | 35  |\n",
    "| lr                | 0.00015  |\n",
    "\n",
    "I-M10:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['crustaceans']  |\n",
    "| endpoints         | ['EC10','NOEC']  |\n",
    "| effects           | ['MOR','DVP','ITX','REP','MPH','POP']  |\n",
    "| epochs            | 35  |\n",
    "| lr                | 0.0005  |\n",
    "\n",
    "I-M5010:\n",
    "| **config variable**   | **value**  |\n",
    "|-------------------|---|\n",
    "| species_groups    | ['crustaceans']  |\n",
    "| endpoints         | ['EC50','EC10','NOEC']  |\n",
    "| effects           | ['MOR','DVP','ITX','REP','MPH','POP']  |\n",
    "| epochs            | 35  |\n",
    "| lr                | 0.0002  |\n",
    "\n",
    "\n",
    "Change the config below according to the specifications above. Values that should change are marked by a #* comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pE_rJu5JRMCZ"
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# TRAINING ######################################\n",
    "config['batch_size'] = 512     \n",
    "config['epochs'] = 35 #*      \n",
    "config['lr'] = 0.0002 #*\n",
    "config['seed'] = 42            \n",
    "config['max_token_length'] = 100\n",
    "config['sampling_procedure'] = 'WRS_sqrt'\n",
    "config['sampler_weight_args'] = ['SMILES_Canonical_RDKit','effect','endpoint']\n",
    "config['optimizer'] = 'AdamW'\n",
    "config['loss_fun'] = 'L1Loss'\n",
    "\n",
    "# MODEL ############################################\n",
    "config['pretrained_model'] = \"seyonec/PubChem10M_SMILES_BPE_450k\"\n",
    "config['n_hidden_layers'] = 3\n",
    "config['hidden_layer_size'] = [700, 500, 300]\n",
    "config['dropout'] = 0.2\n",
    "config['inputs']=['SMILES_Canonical_RDKit', 'Duration_Value', 'OneHotEnc_concatenated']\n",
    "config['label'] = 'mgperL'\n",
    "config['species_classes'] = []\n",
    "config['reinit_n_layers'] = 0\n",
    "\n",
    "# MODIFICATIONS ###########################################\n",
    "config['n_frozen_layers'] = 0 \n",
    "config['freeze_embedding'] = False\n",
    "config['add_roberta_layer'] = False\n",
    "config['use_cls'] = True\n",
    "\n",
    "# DATA #######################################################\n",
    "config['conc_thresh'] = 500\n",
    "config['species_groups'] = ['algae'] #*\n",
    "config['endpoints'] = ['EC50','EC10','NOEC'] #*\n",
    "config['effects'] = ['POP'] #*\n",
    "config['dataset'] = 'large'\n",
    "config['concentration_sign'] = '='\n",
    "config['log_data'] = True\n",
    "\n",
    "if config['n_hidden_layers'] != len(config['hidden_layer_size']):\n",
    "    print('You are not using all layers!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Class(object):\n",
    "    def __init__(self, my_dict):\n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Dict2Class(config)\n",
    "\n",
    "# Config is a variable that holds and saves hyperparameters and inputs\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    torch.manual_seed(seed) # pytorch random seed\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SetSeed(config.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.pretrained_model)\n",
    "\n",
    "chemberta = AutoModel.from_pretrained(config.pretrained_model)\n",
    "\n",
    "print(f'Trainable parameters: {chemberta.num_parameters()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(data, config):\n",
    "    # Preprocesses data for training\n",
    "    processor = PreprocessData(dataframe=data)\n",
    "\n",
    "    processor.FilterData(\n",
    "        concentration_thresh=config.conc_thresh,\n",
    "        endpoint=config.endpoints,\n",
    "        effect=config.effects,\n",
    "        species_groups=config.species_groups,\n",
    "        log_data=True,\n",
    "        concentration_sign=config.concentration_sign)\n",
    "\n",
    "    processor.GetPubchemCID(drop_missing_entries=False)\n",
    "    processor.GetMetadata(list_of_metadata=['cmpdname'])\n",
    "    processor.GetCanonicalSMILES()\n",
    "    processor.ConcatenateOneHotEnc(list_of_endpoints=config.endpoints, list_of_effects=config.effects)\n",
    "\n",
    "    data = processor.dataframe\n",
    "    # Get the number of neurons needed for one hot encoding\n",
    "    fc1 = len(data.OneHotEnc_concatenated.iloc[0])\n",
    "    \n",
    "    return data, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DVKU14BRMCf",
    "outputId": "4cf79d6b-4bf7-4afb-a061-fb8f2d692d06"
   },
   "outputs": [],
   "source": [
    "datadir = '../data/development/'\n",
    "data = pd.read_excel(datadir+'Preprocessed_complete_data_2023.xlsx', sheet_name='dataset')\n",
    "data, fc1 = GetData(data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZnEXOn6RMCi",
    "outputId": "398ce7c0-98b7-4af9-ad2e-0b165ec19634"
   },
   "outputs": [],
   "source": [
    "config.fc1 = fc1\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5I5JV_QXT4y",
    "outputId": "c46e7135-152e-4b25-a8f4-94f8d35d5bf6"
   },
   "outputs": [],
   "source": [
    "# Build Pytorch train dataloader\n",
    "# test_size = 0 ensures entire dataset used as training set\n",
    "DataLoaders = BuildDataLoader_with_trainval_ratio(\n",
    "                                    df = data, \n",
    "                                    wandb_config = config,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed,\n",
    "                                    test_size = 0,\n",
    "                                    tokenizer = tokenizer)\n",
    "        \n",
    "train_dataloader = DataLoaders.BuildTrainingLoader(sampler_choice=config.sampling_procedure, num_workers=2, weight_args=config.sampler_weight_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV_2rcLrRMCo"
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:54.075572Z",
     "iopub.status.busy": "2022-05-04T11:40:54.074950Z",
     "iopub.status.idle": "2022-05-04T11:40:54.092928Z",
     "shell.execute_reply": "2022-05-04T11:40:54.092258Z",
     "shell.execute_reply.started": "2022-05-04T11:40:54.075535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model (consisting of a DNN(-module) and ChemBERTa)\n",
    "dnn_module = DNN_module(\n",
    "                        one_hot_enc_len=fc1,\n",
    "                        n_hidden_layers=config.n_hidden_layers,\n",
    "                        layer_sizes=config.hidden_layer_size,\n",
    "                        dropout=config.dropout)\n",
    "\n",
    "model = TRIDENT(roberta=chemberta, dnn=dnn_module)\n",
    "\n",
    "model = Modify_architecture(model).FreezeModel(model, config.n_frozen_layers, config.freeze_embedding)\n",
    "model = Modify_architecture(model).ReinitializeEncoderLayers(model, reinit_n_layers=config.reinit_n_layers)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbr1fJlDRMCq"
   },
   "source": [
    "## Define train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:58.430879Z",
     "iopub.status.busy": "2022-05-04T11:40:58.425461Z",
     "iopub.status.idle": "2022-05-04T11:40:59.477198Z",
     "shell.execute_reply": "2022-05-04T11:40:59.476376Z",
     "shell.execute_reply.started": "2022-05-04T11:40:58.430838Z"
    },
    "id": "4BFUym5kRMCq"
   },
   "outputs": [],
   "source": [
    "# function to train the model on epoch\n",
    "def train(args, model, dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    total_loss = 0\n",
    "    total_preds=[]\n",
    "    total_labels=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        # Extract batch samples\n",
    "        batch = [r.to(device) for r in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict batch\n",
    "        preds, _ = model(sent_id, mask, duration, onehot)\n",
    "\n",
    "        # Calculate batch loss\n",
    "        loss = loss_fun(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradient to prevent exploding gradients and update weights\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log batch results\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        total_labels.append(labels)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Training Batch Loss\": loss.item(),\n",
    "            \"Learning Rate\": optimizer.param_groups[0][\"lr\"], \n",
    "            'training batch': batch_num[0]\n",
    "        })\n",
    "        batch_num[0] += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    median_loss = np.median(abs(total_preds - total_labels))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training Loss function\": avg_loss,\n",
    "        \"Training Mean Loss\": np.mean(abs(total_preds - total_labels)), \n",
    "        'training epoch': epoch,\n",
    "        \"Training Median Loss\": np.median(abs(total_preds - total_labels)),\n",
    "        \"Training RMSE Loss\": np.sqrt(np.mean((total_labels-total_preds)**2)),\n",
    "        'global_step': global_step})\n",
    "    \n",
    "    return avg_loss, median_loss, total_preds, total_labels, batch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s5W5wHbRMCr"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Layer wise Learning Rate Decay\n",
    "model_parameters = Modify_architecture(model).LLRD(model, init_lr = config.lr)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_parameters, lr=config.lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*config.epochs*len(train_dataloader), num_training_steps=config.epochs*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:59.493865Z",
     "iopub.status.busy": "2022-05-04T11:40:59.493573Z",
     "iopub.status.idle": "2022-05-04T11:40:59.509457Z",
     "shell.execute_reply": "2022-05-04T11:40:59.508631Z",
     "shell.execute_reply.started": "2022-05-04T11:40:59.493830Z"
    }
   },
   "outputs": [],
   "source": [
    "if config.loss_fun == 'L1Loss':\n",
    "    loss_fun = nn.L1Loss()\n",
    "elif config.loss_fun == 'MSELoss':\n",
    "    loss_fun = nn.MSELoss()\n",
    "    \n",
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set name of files (combination of endpoints and species_group)\n",
    "if len(config.endpoints) == 1:\n",
    "    name=f'EC50_{config.species_groups[0]}'\n",
    "elif len(config.endpoints) == 2:\n",
    "    name=f'EC10_{config.species_groups[0]}'\n",
    "elif len(config.endpoints) == 3:\n",
    "    name=f'EC50EC10_{config.species_groups[0]}_withoverlap' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'../TRIDENT/final_model_2023_{name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save fine-tuned ChemBERTa and DNN-module\n",
    "def save_ckp(model, checkpoint_dir):\n",
    "    torch.save(model.dnn.state_dict(), checkpoint_dir+'_dnn_saved_weights.pt')\n",
    "    torch.save(model.roberta.state_dict(), checkpoint_dir+'_roberta_saved_weights.pt')\n",
    "    #wandb.save(checkpoint_dir+'_dnn_saved_weights.pt')\n",
    "    #wandb.save(checkpoint_dir+'_roberta_saved_weights.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:59.513042Z",
     "iopub.status.busy": "2022-05-04T11:40:59.512839Z",
     "iopub.status.idle": "2022-05-04T13:06:09.580905Z",
     "shell.execute_reply": "2022-05-04T13:06:09.580257Z",
     "shell.execute_reply.started": "2022-05-04T11:40:59.513018Z"
    },
    "id": "yrm_iZrDRMCr",
    "outputId": "412dd8d3-4ded-46f3-b798-72bbf9775976"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_val_loss_norm = float('inf')\n",
    "batch_num = [0,0]\n",
    "global_step = 0\n",
    "\n",
    "# Time training\n",
    "start_time = time.time()\n",
    "\n",
    "# Run training epochs\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, config.epochs))\n",
    "    avg_loss, median_loss, total_preds, total_labels, batch_num = train(config, model, train_dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step)\n",
    "    # No validation epochs since this is the final model\n",
    "    print(f'\\nTraining Loss: {median_loss:.3f}')\n",
    "\n",
    "train_time = (time.time() - start_time)/60\n",
    "\n",
    "wandb.log({'Total train time (min)': train_time,\n",
    "            'epoch time (s)': train_time/config.epochs*60})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ckp(model, save_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate resulting model\n",
    "NOTE: Will be partially overfitted to training data (not a problem, new chemicals will still have the accuracy presented in publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new pytorch dataset and dataloader manually\n",
    "dataset = DataLoaders.BuildDataset(DataLoaders.train)\n",
    "sampler = SequentialSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=512, collate_fn=DataLoaders.collator, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate dropout\n",
    "model.eval()\n",
    "\n",
    "# predict the entire training set again and save CLS-embeddings\n",
    "results = DataLoaders.train.copy()\n",
    "predictions = []\n",
    "cls_embeddings = []\n",
    "for step, batch in enumerate(tqdm(dataloader)):\n",
    "    batch = [r.to(device) for r in batch.values()]\n",
    "    sent_id, mask, duration, onehot, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict \n",
    "        preds, cls = model(sent_id, mask, duration, onehot)\n",
    "    predictions.append(preds.detach().cpu().numpy())\n",
    "    cls_embeddings.append(cls.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['preds'] = np.concatenate(predictions, axis=0)\n",
    "results['CLS_embeddings'] = np.concatenate(cls_embeddings, axis=0).tolist()\n",
    "results['residuals'] = results.mgperL-results.preds\n",
    "results['absolute_error'] = abs(results.mgperL-results.preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results locally\n",
    "results.to_pickle(f'../data/results/{name}_final_model_training_data_RDkit.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to weights and biases\n",
    "art = wandb.Artifact(\n",
    "            f\"Training_data_final_model_{name}\", type=\"results_dataset\",\n",
    "            description=f\"{name}\",\n",
    "            metadata={\"source\": \"Preprocessed_complete_data.xlsx\",\n",
    "                      \"sizes\": len(results)})\n",
    "\n",
    "art.add_file(local_path=f'../data/results/{name}_final_model_training_data_RDkit.zip')\n",
    "\n",
    "wandb.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save normalized results locally\n",
    "results['labels'] = results.mgperL\n",
    "results_normalized = CalculateWeightedAverage(results)\n",
    "results_normalized.to_pickle(f'../data/results/{name}_weighted_Avg_Training_data_final_model.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save normalized results to weights and biases\n",
    "art = wandb.Artifact(\n",
    "            f\"Weighted_Avg_Training_data_final_model_{name}\", type=\"weighted_results_dataset\",\n",
    "            description=f\"{name}\",\n",
    "            metadata={\"source\": \"Training_data_final_model_{name}\",\n",
    "                      \"sizes\": len(results_normalized)})\n",
    "\n",
    "art.add_file(local_path=f'../data/results/{name}_weighted_Avg_Training_data_final_model.zip')\n",
    "\n",
    "wandb.log_artifact(art)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T13:06:21.882188Z",
     "iopub.status.busy": "2022-05-04T13:06:21.881712Z",
     "iopub.status.idle": "2022-05-04T13:06:24.696364Z",
     "shell.execute_reply": "2022-05-04T13:06:24.694897Z",
     "shell.execute_reply.started": "2022-05-04T13:06:21.882149Z"
    },
    "id": "QzSBatqMRMCt"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(results_normalized, x='mgperL', y='preds', hover_data=['SMILES_Canonical_RDKit'], trendline='ols', trendline_color_override=\"black\")\n",
    "\n",
    "fig.update_traces(marker=dict(line_width=0.5, line_color='Black'))\n",
    "fig.update_yaxes(title_text=\"Predicted Concentration [Log10(mg/L)]\", range=[-4,4])\n",
    "fig.update_xaxes(title_text=\"Actual Concentration [Log10(mg/L)]\", range=[-4,4])\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='grey')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='grey')\n",
    "fig.update_layout(\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title=f'Predictions vs. actual labels, one per SMILES n={len(results_normalized)}')\n",
    "\n",
    "fig.update_layout(\n",
    "        font_family='Serif',\n",
    "        font=dict(size=16), \n",
    "        plot_bgcolor='rgba(0, 0, 0, 0)')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "wandb.log({'Prediction vs target (one per chemical)': fig}, commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers2",
   "language": "python",
   "name": "transformers"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
