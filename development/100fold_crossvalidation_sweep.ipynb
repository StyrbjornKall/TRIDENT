{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and launch ten times repeated 10-fold cross-validation on multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T12:20:06.573146Z",
     "iopub.status.busy": "2022-04-07T12:20:06.572791Z",
     "iopub.status.idle": "2022-04-07T12:20:06.594684Z",
     "shell.execute_reply": "2022-04-07T12:20:06.594057Z",
     "shell.execute_reply.started": "2022-04-07T12:20:06.573114Z"
    },
    "id": "MjtASeFVRMCW",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from development_utils.preprocessing.Get_data_for_model import *\n",
    "from development_utils.training.Build_data_for_pytorch import *\n",
    "from development_utils.training.Build_model_pytorch import *\n",
    "from development_utils.training.PerformanceCalculations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.327439Z",
     "iopub.status.busy": "2022-04-01T05:46:31.326981Z",
     "iopub.status.idle": "2022-04-01T05:46:31.394170Z",
     "shell.execute_reply": "2022-04-01T05:46:31.393370Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.327400Z"
    },
    "id": "d60ceda8",
    "outputId": "93f57e48-3fb9-49f9-f4b0-48d51e958abd",
    "papermill": {
     "duration": 0.087271,
     "end_time": "2022-03-04T15:15:20.579086",
     "exception": false,
     "start_time": "2022-03-04T15:15:20.491815",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs on node: NVIDIA A100-SXM4-40GB\n",
      "Number of GPUs available: 1\n",
      "Using cuda:0 device\n",
      "42.35 Gb free on CUDA\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GPUinfo(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ENTITYNAME = 'ecotoxformer'\n",
    "PROJECTNAME = '100Fold_CV_RDKit'\n",
    "SWEEPID = 'b9s38isu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.395755Z",
     "iopub.status.busy": "2022-04-01T05:46:31.395495Z",
     "iopub.status.idle": "2022-04-01T05:46:37.015133Z",
     "shell.execute_reply": "2022-04-01T05:46:37.014459Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.395718Z"
    },
    "id": "826933f3",
    "outputId": "3657c92a-871e-4cd8-d2c6-c1b913230444",
    "papermill": {
     "duration": 1.576782,
     "end_time": "2022-03-04T15:15:22.19011",
     "exception": true,
     "start_time": "2022-03-04T15:15:20.613328",
     "status": "failed"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def GetData(data, config):\n",
    "    processor = PreprocessData(dataframe=data)\n",
    "\n",
    "    processor.FilterData(\n",
    "        concentration_thresh=config.conc_thresh,\n",
    "        endpoint=config.endpoints,\n",
    "        effect=config.effects,\n",
    "        species_groups=config.species_groups,\n",
    "        log_data=True,\n",
    "        concentration_sign=config.concentration_sign)\n",
    "\n",
    "    processor.GetPubchemCID()\n",
    "    processor.GetMetadata(['cmpdname'])\n",
    "    processor.GetCanonicalSMILES()\n",
    "    processor.GetOneHotEndpoint(config.endpoints)\n",
    "    processor.GetOneHotEffect(config.effects)\n",
    "    processor.ConcatenateOneHotEnc()\n",
    "\n",
    "    data = processor.dataframe\n",
    "    fc1 = len(data.OneHotEnc_concatenated.iloc[0])\n",
    "    \n",
    "    return data, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    torch.manual_seed(seed) # pytorch random seed\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def GetLayers(config):\n",
    "    if config.n_hidden_layers == 1:\n",
    "        return [config.layer_1]\n",
    "    elif config.n_hidden_layers == 2:\n",
    "        return [config.layer_1, config.layer_2]\n",
    "    elif config.n_hidden_layers == 3:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3]\n",
    "    elif config.n_hidden_layers == 4:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3, config.layer_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def RunTrainingEpochs(data, folds, fold_id, config, fc1, global_step):\n",
    "    \n",
    "    chemberta = AutoModel.from_pretrained(config.base_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "    DataLoaders = BuildDataLoader_KFold(\n",
    "                                    df = data,\n",
    "                                    folds = folds,\n",
    "                                    fold_id=fold_id, \n",
    "                                    variables = config.inputs,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed, \n",
    "                                    tokenizer = tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoaders.BuildTrainingLoader(sampler_choice=config.sampling_procedure, num_workers=2, weight_args=['SMILES','COMBINED_effect','COMBINED_endpoint'])\n",
    "    val_dataloader = DataLoaders.BuildValidationLoader(sampler_choice='SequentialSampler', num_workers=2)\n",
    "    print('Successfully built dataloader')\n",
    "    print(f'SMILES overlap train/validation: {len(set(DataLoaders.train.SMILES.tolist())&set(DataLoaders.val.SMILES.tolist()))}')\n",
    "    \n",
    "    wandb.log({\"Training df\": wandb.Table(dataframe=DataLoaders.train)})\n",
    "######## MODEL ##################################################################################\n",
    "    dnn_module = DNN_module(\n",
    "                        one_hot_enc_len=fc1,\n",
    "                        n_hidden_layers=config.n_hidden_layers,\n",
    "                        layer_sizes=GetLayers(config),\n",
    "                        dropout=config.dropout,\n",
    "                        activation='ReLU')\n",
    "\n",
    "    model = fishbAIT(roberta=chemberta, dnn=dnn_module)\n",
    "\n",
    "    model = Modify_architecture(model).FreezeModel(model, config.n_frozen_layers, config.freeze_embedding)\n",
    "    model = Modify_architecture(model).ReinitializeEncoderLayers(model, reinit_n_layers=config.reinit_n_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "######## TRAINING CONFIG ##################################################################################        \n",
    "    model_parameters = Modify_architecture(model).LLRD(model, init_lr = config.lr)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model_parameters, lr=config.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*config.epochs*len(train_dataloader), num_training_steps=config.epochs*len(train_dataloader))\n",
    "    print('Successfully built optimizer')\n",
    "\n",
    "    if config.loss_fun == 'MSELoss':\n",
    "        loss_fun = nn.MSELoss()\n",
    "    else:\n",
    "        loss_fun = nn.L1Loss()\n",
    "\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_loss_norm = np.inf\n",
    "    \n",
    "    batch_num = [0,0]\n",
    "\n",
    "######## RUN TRAINING ##################################################################################\n",
    "\n",
    "    avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, -1, global_step-1)\n",
    "\n",
    "    if median_loss < best_val_loss:\n",
    "        best_val_loss = median_loss\n",
    "    \n",
    "    print(\"\\nRunning epochs...\")\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "\n",
    "        avg_loss, median_loss, total_preds, total_labels, batch_num = train(config, model, train_dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        if median_loss_norm < best_val_loss_norm:\n",
    "            best_val_loss = median_loss\n",
    "            best_val_loss_norm = median_loss_norm\n",
    "            best_validation_results = val_results\n",
    "            best_validation_mean_norm_loss = avg_loss_norm\n",
    "\n",
    "        wandb.log({'Best Validation Median Loss': best_val_loss,\n",
    "                    'Best Validation Median Loss Normalized': best_val_loss_norm,\n",
    "                    'Best Validation Mean Loss Normalized': best_validation_mean_norm_loss,\n",
    "                    'global_step': global_step})\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "    wandb.log({\"Best Validation Results\": wandb.Table(dataframe=best_validation_results)})\n",
    "        \n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    del loss_fun\n",
    "    del chemberta\n",
    "    del tokenizer\n",
    "\n",
    "    return best_val_loss, best_val_loss_norm, best_validation_mean_norm_loss, global_step, best_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(args, model, dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step):\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    total_loss = 0\n",
    "    total_preds=[]\n",
    "    total_labels=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = [r.to(device) for r in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(sent_id, mask, duration, onehot)\n",
    "        loss = loss_fun(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        total_labels.append(labels)\n",
    "        \n",
    "        \n",
    "        wandb.log({\n",
    "            \"Training Batch Loss\": loss.item(),\n",
    "            \"Learning Rate\": optimizer.param_groups[0][\"lr\"], \n",
    "            'training batch': batch_num[0]\n",
    "        })\n",
    "        batch_num[0] += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    median_loss = np.median(abs(total_preds - total_labels))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training Loss function\": avg_loss,\n",
    "        \"Training Mean Loss\": np.mean(abs(total_preds - total_labels)), \n",
    "        'training epoch': epoch,\n",
    "        \"Training Median Loss\": np.median(abs(total_preds - total_labels)),\n",
    "        \"Training RMSE Loss\": np.sqrt(np.mean((total_labels-total_preds)**2)),\n",
    "        'global_step': global_step})\n",
    "    \n",
    "    return avg_loss, median_loss, total_preds, total_labels, batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, dataloader, dataset, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    val_results = dataset.copy()\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = [t.to(device) for t in batch.values()]\n",
    "\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(sent_id, mask, duration, onehot)\n",
    "                loss = loss_fun(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "        batch_num[1] += 1\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    val_results['labels'] = total_labels\n",
    "    val_results['preds'] = total_preds\n",
    "    val_results['residuals'] = val_results.labels-val_results.preds\n",
    "    val_results['L1Error'] = abs(total_labels - total_preds)\n",
    "    median_loss = val_results.L1Error.median()\n",
    "    val_results_normalized = CalculateWeightedAverage(val_results)\n",
    "    median_loss_norm = abs(val_results_normalized.residuals).median()\n",
    "    avg_loss_norm = abs(val_results_normalized.residuals).mean()\n",
    "    wandb.log({\n",
    "        \"Validation Loss function\": avg_loss,\n",
    "        \"Validation Mean Loss\": val_results.L1Error.mean(),\n",
    "        \"Validation Median Loss\": median_loss,\n",
    "        \"Validation Loss Normalized\": median_loss_norm,\n",
    "        \"Validation Mean Loss Normalized\": avg_loss_norm,\n",
    "        \"Validation RMSE Loss Normalized\": np.sqrt(((val_results_normalized.labels - val_results_normalized.preds)**2).mean()),\n",
    "        'validation epoch': epoch,\n",
    "        'global_step': global_step\n",
    "        })\n",
    "        \n",
    "    return avg_loss, avg_loss_norm, median_loss, median_loss_norm, total_preds, batch_num, val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def trainer(config=None):\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    SetSeed(42)\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "        sweepconfig = wandb.config\n",
    "\n",
    "        datadir = '../data/'\n",
    "\n",
    "    ######## DATA ##################################################################################\n",
    "        data = pd.read_csv(datadir+'MSc_Thesis_Cleaned.csv', encoding='windows-1252')\n",
    "        data, fc1 = GetData(data, sweepconfig)\n",
    "        print('Successfully loaded data')\n",
    "\n",
    "        folds = Make_KFolds().Split(data[sweepconfig.inputs[0]], k_folds=sweepconfig.k_folds, seed=sweepconfig.seed)\n",
    "        print('Successfully built folds')\n",
    "        name = wandb.run.name\n",
    "        \n",
    "        print(f'\\n Running fold {sweepconfig.fold_id} using seed {sweepconfig.seed}')\n",
    "        global_step = 0\n",
    "\n",
    "        Best_Validation_Median_Loss, Best_Validation_Median_Loss_Normalized, Best_Validation_Mean_Loss_Normalized, global_step, best_validation_results = RunTrainingEpochs(data, folds, sweepconfig.fold_id, sweepconfig, fc1, global_step)\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s5W5wHbRMCr"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o3hvtaok with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: ChemBERTa+DNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model: ['seyonec/PubChem10M_SMILES_BPE_450k']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconc_thresh: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconcentration_sign: =\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: large\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffects: ['MOR', 'DVP', 'POP', 'MPH', 'ITX', 'REP', 'GRO']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tendpoints: ['EC50', 'EC10', 'NOEC']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfold_id: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_embedding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinputs: ['SMILES', 'COMBINED_Duration_Value', 'OneHotEnc_concatenated']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk_folds: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlabel: COMBINED_mgperL\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_1: 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_2: 480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_3: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fun: L1Loss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_token_length: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_frozen_layers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treinit_n_layers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling_procedure: WRS_sqrt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tspecies_classes: []\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tspecies_groups: ['fish']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_cls: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m (\u001b[33mecotoxformer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mimer/NOBACKUP/groups/snic2022-22-552/skall/wandb/wandb/run-20220928_151256-o3hvtaok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ecotoxformer/100Fold_CV/runs/o3hvtaok\" target=\"_blank\">giddy-sweep-1</a></strong> to <a href=\"https://wandb.ai/ecotoxformer/100Fold_CV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ecotoxformer/100Fold_CV/sweeps/t9d7rspz\" target=\"_blank\">https://wandb.ai/ecotoxformer/100Fold_CV/sweeps/t9d7rspz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 221 entries from dataframe due to SMILES not having CID\n",
      "Renamed EC10 *NOEC* in 128 positions\n",
      "Did not return onehotencoding for Species groups. Why? You specified only one species group.\n",
      "Did not return onehotencoding for Species classes. Why? No specified classes.\n",
      "Successfully loaded data\n",
      "Successfully built folds\n",
      "\n",
      " Running fold 1 using seed 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/Arch/software/SciPy-bundle/2020.11-fosscuda-2020b/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "Some weights of the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built training dataloader with 61520 samples\n",
      "Built validation dataloader with 8747 samples\n",
      "Successfully built dataloader\n",
      "SMILES overlap train/validation: 7\n",
      "Successfully built optimizer\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e27483f448340edae15515aea92ffa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a91137aeb641acbb2b11e4912179cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c009d23377744db9a9e15fb668d8abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_id = '6fscgxpi'\n",
    "wandb.agent('ecotoxformer/100Fold_CV_RDKit/'+sweep_id, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
