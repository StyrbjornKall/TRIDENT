{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch K-fold Cross-Validation SWEEP on multiple GPUs\n",
    "\n",
    "\n",
    "**Purpose:** This script performs a K-fold cross-validation once the model architecture is fixed and was used to determine the following:\n",
    "1. 10 X 10 - fold cross-validation on M50, M10 and M50/10 (used for accuracy metric in publication)\n",
    "    - variable parameters: [fold_id, seed]\n",
    "    - config file: `100fold_CV_EC50.yaml`, `100fold_CV_EC10.yaml` and `100fold_CV_EC5010.yaml`\n",
    "\n",
    "**Dependency:** `hyperparameter_sweep.ipynb`. This script uses model configurations found from several (hyper)parameter sweeps (`hyperparameter_sweep.ipynb`).\n",
    "\n",
    "**Consecutive scripts:** After running this script the following scripts may be executed. `Train_final_models.ipynb`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MjtASeFVRMCW"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/cephyr/users/skall/Alvis/TRIDENT/development/development_utils/')\n",
    "import os\n",
    "os.chdir('/cephyr/users/skall/Alvis/TRIDENT/development/')\n",
    "\n",
    "from development_utils.preprocessing.Get_data_for_model import PreprocessData\n",
    "from development_utils.training.Build_Pytorch_Dataset_and_DataLoader import BuildDataLoader_KFold, Make_KFolds\n",
    "from development_utils.training.Build_Pytorch_model import TRIDENT, DNN_module, GPUinfo, Modify_architecture\n",
    "from development_utils.training.PerformanceCalculations import CalculateWeightedAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d60ceda8",
    "outputId": "93f57e48-3fb9-49f9-f4b0-48d51e958abd",
    "papermill": {
     "duration": 0.087271,
     "end_time": "2022-03-04T15:15:20.579086",
     "exception": false,
     "start_time": "2022-03-04T15:15:20.491815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs on node: Tesla T4\n",
      "Number of GPUs available: 1\n",
      "Using cuda:0 device\n",
      "15.69 Gb free on CUDA\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUinfo(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITYNAME = 'ecotoxformer'\n",
    "PROJECTNAME = '100Fold_CV_RDKit_invertebrates'\n",
    "SWEEPID = 'u1c9loib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "826933f3",
    "outputId": "3657c92a-871e-4cd8-d2c6-c1b913230444",
    "papermill": {
     "duration": 1.576782,
     "end_time": "2022-03-04T15:15:22.19011",
     "exception": true,
     "start_time": "2022-03-04T15:15:20.613328",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(data, config):\n",
    "    # Preprocesses data for training\n",
    "    processor = PreprocessData(dataframe=data)\n",
    "\n",
    "    processor.FilterData(\n",
    "        concentration_thresh=config.conc_thresh,\n",
    "        endpoint=config.endpoints,\n",
    "        effect=config.effects,\n",
    "        species_groups=config.species_groups,\n",
    "        log_data=True,\n",
    "        concentration_sign=config.concentration_sign)\n",
    "\n",
    "    processor.GetPubchemCID(drop_missing_entries=False)\n",
    "    processor.GetMetadata(list_of_metadata=['cmpdname'])\n",
    "    processor.GetCanonicalSMILES()\n",
    "    processor.ConcatenateOneHotEnc(list_of_endpoints=config.endpoints, list_of_effects=config.effects)\n",
    "\n",
    "    data = processor.dataframe\n",
    "    data['CAS'] = data.CAS.astype(str)\n",
    "    # Get the number of neurons needed for one hot encoding\n",
    "    fc1 = len(data.OneHotEnc_concatenated.iloc[0])\n",
    "\n",
    "    print(f'\\n Total number of samples after preprocessing: {len(data)}')\n",
    "    \n",
    "    return data, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    # Sets random seed for deterministic training\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLayers(config):\n",
    "    # Function to buil list of layer sizes\n",
    "    if config.n_hidden_layers == 1:\n",
    "        return [config.layer_1]\n",
    "    elif config.n_hidden_layers == 2:\n",
    "        return [config.layer_1, config.layer_2]\n",
    "    elif config.n_hidden_layers == 3:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3]\n",
    "    elif config.n_hidden_layers == 4:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3, config.layer_4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunTrainingEpochs(data, folds, fold_id, config, fc1, global_step):\n",
    "    # Load ChemBERTa\n",
    "    chemberta = AutoModel.from_pretrained(config.base_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "    # Build Pytorch train and validation dataloader based on fold\n",
    "    DataLoaders = BuildDataLoader_KFold(\n",
    "                                    df = data[0],\n",
    "                                    folds = folds[0],\n",
    "                                    fold_id=fold_id,\n",
    "                                    wandb_config = config,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed, \n",
    "                                    tokenizer = tokenizer)\n",
    "    DataLoaders_ec10 = BuildDataLoader_KFold(\n",
    "                                    df = data[1],\n",
    "                                    folds = folds[1],\n",
    "                                    fold_id=fold_id, \n",
    "                                    wandb_config = config,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed, \n",
    "                                    tokenizer = tokenizer) \n",
    "    \n",
    "    # Concatenate folds from ec10 and ec50 to allow chemical overlap\n",
    "    DataLoaders.train = pd.concat([DataLoaders.train, DataLoaders_ec10.train], axis=0) \n",
    "    DataLoaders.val = pd.concat([DataLoaders.val, DataLoaders_ec10.val], axis=0) \n",
    "    \n",
    "    train_dataloader = DataLoaders.BuildTrainingLoader(sampler_choice=config.sampling_procedure, num_workers=2, weight_args=['SMILES_Canonical_RDKit','effect','endpoint'])\n",
    "    val_dataloader = DataLoaders.BuildValidationLoader(sampler_choice='SequentialSampler', num_workers=2)\n",
    "    print('Successfully built dataloader')\n",
    "    print(f'SMILES overlap train/validation: {len(set(DataLoaders.train.SMILES_Canonical_RDKit.tolist())&set(DataLoaders.val.SMILES_Canonical_RDKit.tolist()))}')\n",
    "    \n",
    "    wandb.log({\"Training df\": wandb.Table(dataframe=DataLoaders.train)})\n",
    "######## MODEL ##################################################################################\n",
    "    # Build the model (consisting of a DNN(-module) and ChemBERTa)\n",
    "    dnn_module = DNN_module(\n",
    "                        one_hot_enc_len=fc1,\n",
    "                        n_hidden_layers=config.n_hidden_layers,\n",
    "                        layer_sizes=GetLayers(config),\n",
    "                        dropout=config.dropout)\n",
    "\n",
    "    model = TRIDENT(roberta=chemberta, dnn=dnn_module)\n",
    "\n",
    "    model = Modify_architecture(model).FreezeModel(model, config.n_frozen_layers, config.freeze_embedding)\n",
    "    model = Modify_architecture(model).ReinitializeEncoderLayers(model, reinit_n_layers=config.reinit_n_layers)\n",
    "    model = model.to(device)\n",
    "    print('Successfully built model\\n')\n",
    "\n",
    "######## TRAINING CONFIG ##################################################################################        \n",
    "    # Apply Layer wise Learning Rate Decay\n",
    "    model_parameters = Modify_architecture(model).LLRD(model, init_lr = config.lr)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model_parameters, lr=config.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*config.epochs*len(train_dataloader), num_training_steps=config.epochs*len(train_dataloader))\n",
    "    print('Successfully built optimizer')\n",
    "\n",
    "    if config.loss_fun == 'MSELoss':\n",
    "        loss_fun = nn.MSELoss()\n",
    "    else:\n",
    "        loss_fun = nn.L1Loss()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_loss_norm = np.inf\n",
    "    \n",
    "    batch_num = [0,0]\n",
    "\n",
    "    if len(config.endpoints) == 1:\n",
    "        endpoint='EC50'\n",
    "    elif len(config.endpoints) == 2:\n",
    "        endpoint='EC10'\n",
    "    elif len(config.endpoints) == 3:\n",
    "        endpoint='EC50EC10'\n",
    "\n",
    "    save_name = f'/mimer/NOBACKUP/groups/snic2022-22-552/skall/100_foldCV_models_and_data_{endpoint}/fold{config.fold_id}_seed{config.seed}_model'\n",
    "    def save_ckp(model, checkpoint_dir):\n",
    "        torch.save(model.dnn.state_dict(), checkpoint_dir+'_dnn_saved_weights.pt')\n",
    "        torch.save(model.roberta.state_dict(), checkpoint_dir+'_roberta_saved_weights.pt')\n",
    "\n",
    "######## RUN TRAINING ##################################################################################\n",
    "    # Log initial validation loss\n",
    "    avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, -1, global_step-1)\n",
    "\n",
    "    if median_loss < best_val_loss:\n",
    "        best_val_loss = median_loss\n",
    "    \n",
    "    # Run epochs\n",
    "    print(\"\\nRunning epochs...\")\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "\n",
    "        avg_loss, median_loss, total_preds, total_labels, batch_num = train(config, model, train_dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        # Update and log epoch results\n",
    "        if median_loss_norm < best_val_loss_norm:\n",
    "            best_val_loss = median_loss\n",
    "            best_val_loss_norm = median_loss_norm\n",
    "            best_validation_results = val_results\n",
    "            best_validation_mean_norm_loss = avg_loss_norm\n",
    "            if config.save_model == True:\n",
    "                save_ckp(model, save_name)\n",
    "\n",
    "        wandb.log({'Best Validation Median Loss': best_val_loss,\n",
    "                    'Best Validation Median Loss Normalized': best_val_loss_norm,\n",
    "                    'Best Validation Mean Loss Normalized': best_validation_mean_norm_loss,\n",
    "                    'global_step': global_step})\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "\n",
    "######## SAVE RESULTS ##################################################################################\n",
    "    if config.save_results == True:\n",
    "        #DataLoaders.train.to_csv(save_name+'_trainingdata.csv', index=False)    \n",
    "        #DataLoaders.val.to_csv(save_name+'_validationdata.csv', index=False)   \n",
    "        wandb.log({\"Best Validation Results\": wandb.Table(dataframe=best_validation_results.drop('CLS_embeddings')),\n",
    "                    #\"Training data\": wandb.Table(dataframe=DataLoaders.train),\n",
    "                    #\"Validation data\": wandb.Table(dataframe=DataLoaders.val),\n",
    "                    })\n",
    "\n",
    "    if config.save_final_epoch == True:\n",
    "        wandb.log({\"Validation Results Final Epoch\": wandb.Table(dataframe=val_results)})\n",
    "        \n",
    "######## DELETE FOLD PARAMETERS ##################################################################################\n",
    "    del model\n",
    "    del optimizer\n",
    "    del loss_fun\n",
    "    del chemberta\n",
    "    del tokenizer\n",
    "\n",
    "    return best_val_loss, best_val_loss_norm, best_validation_mean_norm_loss, global_step, best_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model on epoch\n",
    "def train(args, model, dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    total_loss = 0\n",
    "    total_preds=[]\n",
    "    total_labels=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        # Extract batch samples\n",
    "        batch = [r.to(device) for r in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict batch\n",
    "        preds, _ = model(sent_id, mask, duration, onehot)\n",
    "\n",
    "        # Calculate batch loss\n",
    "        loss = loss_fun(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradient to prevent exploding gradients and update weights\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log batch results\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        total_labels.append(labels)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Training Batch Loss\": loss.item(),\n",
    "            \"Learning Rate\": optimizer.param_groups[0][\"lr\"], \n",
    "            'training batch': batch_num[0]\n",
    "        })\n",
    "        batch_num[0] += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    median_loss = np.median(abs(total_preds - total_labels))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training Loss function\": avg_loss,\n",
    "        \"Training Mean Loss\": np.mean(abs(total_preds - total_labels)), \n",
    "        'training epoch': epoch,\n",
    "        \"Training Median Loss\": np.median(abs(total_preds - total_labels)),\n",
    "        \"Training RMSE Loss\": np.sqrt(np.mean((total_labels-total_preds)**2)),\n",
    "        'global_step': global_step})\n",
    "    \n",
    "    return avg_loss, median_loss, total_preds, total_labels, batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to validate the model on epoch\n",
    "def evaluate(model, dataloader, dataset, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize validation array in which to log results\n",
    "    val_results = dataset.copy()\n",
    "    cls_embeddings = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        # Extract batch samples\n",
    "        batch = [t.to(device) for t in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Predict batch\n",
    "            preds, roberta_output = model(sent_id, mask, duration, onehot)\n",
    "\n",
    "            # Calculate batch loss\n",
    "            loss = loss_fun(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log batch results\n",
    "            cls_embeddings.append(roberta_output.detach().cpu().numpy())\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "        batch_num[1] += 1\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    val_results['CLS_embeddings']  = np.concatenate(cls_embeddings, axis=0).tolist()\n",
    "    val_results['labels'] = total_labels\n",
    "    val_results['preds'] = total_preds\n",
    "    val_results['residuals'] = val_results.labels-val_results.preds\n",
    "    val_results['L1Error'] = abs(total_labels - total_preds)\n",
    "    median_loss = val_results.L1Error.median()\n",
    "    val_results_normalized = CalculateWeightedAverage(val_results)\n",
    "    median_loss_norm = abs(val_results_normalized.residuals).median()\n",
    "    avg_loss_norm = abs(val_results_normalized.residuals).mean()\n",
    "    wandb.log({\n",
    "        \"Validation Loss function\": avg_loss,\n",
    "        \"Validation Mean Loss\": val_results.L1Error.mean(),\n",
    "        \"Validation Median Loss\": median_loss,\n",
    "        \"Validation Loss Normalized\": median_loss_norm,\n",
    "        \"Validation Mean Loss Normalized\": avg_loss_norm,\n",
    "        \"Validation RMSE Loss Normalized\": np.sqrt(((val_results_normalized.labels - val_results_normalized.preds)**2).mean()),\n",
    "        'validation epoch': epoch,\n",
    "        'global_step': global_step\n",
    "        })\n",
    "        \n",
    "    return avg_loss, avg_loss_norm, median_loss, median_loss_norm, total_preds, batch_num, val_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(config=None):\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    SetSeed(42)\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, dir='/mimer/NOBACKUP/groups/snic2022-22-552/skall/wandb/'):\n",
    "\n",
    "        # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "        sweepconfig = wandb.config\n",
    "\n",
    "    ######## DATA ##################################################################################\n",
    "        # Load dataframe\n",
    "        datadir = '../data/development/'\n",
    "        data = pd.read_excel(datadir+'Preprocessed_complete_data_2023.xlsx', sheet_name='dataset')\n",
    "        data, fc1 = GetData(data, sweepconfig)\n",
    "        print('Successfully loaded data')\n",
    "        \n",
    "        # Build K-folds based on SMILES (each fold has a unique set of SMILES)\n",
    "        ec10 = data[data.endpoint == 'EC10']\n",
    "        ec50 = data[data.endpoint == 'EC50']\n",
    "\n",
    "        folds_ec10 = Make_KFolds().Split(ec10[sweepconfig.smiles_col_name], k_folds=sweepconfig.k_folds, seed=sweepconfig.seed)\n",
    "        folds_ec50 = Make_KFolds().Split(ec50[sweepconfig.smiles_col_name], k_folds=sweepconfig.k_folds, seed=sweepconfig.seed) \n",
    "        \n",
    "        folds = [folds_ec50, folds_ec10]\n",
    "        data = [ec50, ec10]\n",
    "        print('Successfully built folds')\n",
    "        name = wandb.run.name\n",
    "        \n",
    "        # Run one fold\n",
    "        print(f'\\n Running fold {sweepconfig.fold_id} using seed {sweepconfig.seed}')\n",
    "        global_step = 0\n",
    "\n",
    "        _, _, _, global_step, _ = RunTrainingEpochs(data, folds, sweepconfig.fold_id, sweepconfig, fc1, global_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_s5W5wHbRMCr"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run wandb agent (runs the script)\n",
    "wandb.agent(f'{ENTITYNAME}/{PROJECTNAME}/{SWEEPID}', trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers2",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1514c81f3705f7107573a782eeac851e32c3c00fc4e7dabdb8a1c6e8ef21406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
