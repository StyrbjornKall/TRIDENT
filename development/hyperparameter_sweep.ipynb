{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and launch parameter and hyperparameter SWEEP on multiple GPUs\n",
    "\n",
    "**Purpose:** This script performes 5-fold cross-validation (hyper)parameter sweeps used to run the following sweeps:\n",
    "1. ChemBERTa version and loss function\n",
    "    - variable parameters: [learning rate, loss function, ChemBERTa version]\n",
    "    - config file: `BPEtok10M.yaml` and `SMILEStok1M.yaml`\n",
    "2. Batch size\n",
    "    - variable parameters: [learning rate, batch size]\n",
    "    - config file: `batch_size.yaml`\n",
    "3. M50 (EC50) Model configuration\n",
    "    - variable parameters: [learning rate, number of reinitialized encoders, number of hidden layers, hidden layer sizes]\n",
    "    - config file: `2_hidden_layers_hpsweep.yaml`, `3_hidden_layers_hpsweep.yaml` and `4_hidden_layers_hpsweep.yaml`\n",
    "4. M10 (EC10) Model configuration\n",
    "    - variable parameters: [learning rate, number of reinitialized encoders, number of hidden layers, hidden layer sizes]\n",
    "    - config file: `2_hidden_layers_hpsweep.yaml`, `3_hidden_layers_hpsweep.yaml` and `4_hidden_layers_hpsweep.yaml`\n",
    "5. M5010 (EC50 and EC10 combo) Model configuration\n",
    "    - variable parameters: [learning rate, number of reinitialized encoders, number of hidden layers, hidden layer sizes]\n",
    "    - config file: `2_hidden_layers_hpsweep.yaml`, `3_hidden_layers_hpsweep.yaml` and `4_hidden_layers_hpsweep.yaml`\n",
    "\n",
    "**Dependency:** None (apart from initial model testing)\n",
    "\n",
    "**Consecutive scripts:** After running this script the following scripts may be executed. `Kfold_crossvalidation_sweep.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T12:20:06.573146Z",
     "iopub.status.busy": "2022-04-07T12:20:06.572791Z",
     "iopub.status.idle": "2022-04-07T12:20:06.594684Z",
     "shell.execute_reply": "2022-04-07T12:20:06.594057Z",
     "shell.execute_reply.started": "2022-04-07T12:20:06.573114Z"
    },
    "id": "MjtASeFVRMCW"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from development_utils.preprocessing.Get_data_for_model import PreprocessData\n",
    "from development_utils.training.Build_Pytorch_Dataset_and_DataLoader import BuildDataLoader_KFold, Make_KFolds\n",
    "from development_utils.training.Build_Pytorch_model import ecoCAIT, DNN_module, GPUinfo, Modify_architecture\n",
    "from development_utils.training.PerformanceCalculations import CalculateWeightedAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.327439Z",
     "iopub.status.busy": "2022-04-01T05:46:31.326981Z",
     "iopub.status.idle": "2022-04-01T05:46:31.394170Z",
     "shell.execute_reply": "2022-04-01T05:46:31.393370Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.327400Z"
    },
    "id": "d60ceda8",
    "outputId": "93f57e48-3fb9-49f9-f4b0-48d51e958abd",
    "papermill": {
     "duration": 0.087271,
     "end_time": "2022-03-04T15:15:20.579086",
     "exception": false,
     "start_time": "2022-03-04T15:15:20.491815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs on node: NVIDIA A100-SXM4-40GB\n",
      "Number of GPUs available: 1\n",
      "Using cuda:0 device\n",
      "42.35 Gb free on CUDA\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUinfo(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITYNAME = 'ecotoxformer'\n",
    "PROJECTNAME = 'testing_github'\n",
    "SWEEPID = 'b9s38isu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.395755Z",
     "iopub.status.busy": "2022-04-01T05:46:31.395495Z",
     "iopub.status.idle": "2022-04-01T05:46:37.015133Z",
     "shell.execute_reply": "2022-04-01T05:46:37.014459Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.395718Z"
    },
    "id": "826933f3",
    "outputId": "3657c92a-871e-4cd8-d2c6-c1b913230444",
    "papermill": {
     "duration": 1.576782,
     "end_time": "2022-03-04T15:15:22.19011",
     "exception": true,
     "start_time": "2022-03-04T15:15:20.613328",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T12:20:22.950455Z",
     "iopub.status.busy": "2022-04-07T12:20:22.950204Z",
     "iopub.status.idle": "2022-04-07T12:20:25.431843Z",
     "shell.execute_reply": "2022-04-07T12:20:25.431174Z",
     "shell.execute_reply.started": "2022-04-07T12:20:22.950420Z"
    },
    "id": "tZnEXOn6RMCi",
    "outputId": "398ce7c0-98b7-4af9-ad2e-0b165ec19634"
   },
   "outputs": [],
   "source": [
    "def GetData(data, config):\n",
    "    # Preprocesses data for training\n",
    "    processor = PreprocessData(dataframe=data)\n",
    "\n",
    "    processor.FilterData(\n",
    "        concentration_thresh=config.conc_thresh,\n",
    "        endpoint=config.endpoints,\n",
    "        effect=config.effects,\n",
    "        species_groups=config.species_groups,\n",
    "        log_data=True,\n",
    "        concentration_sign=config.concentration_sign)\n",
    "\n",
    "    processor.GetPubchemCID()\n",
    "    processor.GetMetadata(list_of_metadata=['cmpdname'])\n",
    "    processor.GetCanonicalSMILES()\n",
    "    processor.ConcatenateOneHotEnc(list_of_endpoints=config.endpoints, list_of_effects=config.effects)\n",
    "\n",
    "    data = processor.dataframe\n",
    "    # Get the number of neurons needed for one hot encoding\n",
    "    fc1 = len(data.OneHotEnc_concatenated.iloc[0])\n",
    "    \n",
    "    return data, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    # Sets random seed for deterministic training\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLayers(config):\n",
    "    # Function to buil list of layer sizes\n",
    "    if config.n_hidden_layers == 1:\n",
    "        return [config.layer_1]\n",
    "    elif config.n_hidden_layers == 2:\n",
    "        return [config.layer_1, config.layer_2]\n",
    "    elif config.n_hidden_layers == 3:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3]\n",
    "    elif config.n_hidden_layers == 4:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3, config.layer_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbr1fJlDRMCq"
   },
   "source": [
    "### Train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunTrainingEpochs(data, folds, fold_id, config, fc1, global_step):\n",
    "    \n",
    "    # Load ChemBERTa\n",
    "    chemberta = AutoModel.from_pretrained(config.base_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "    # Build Pytorch train and validation dataloader based on fold\n",
    "    DataLoaders = BuildDataLoader_KFold(\n",
    "                                    df = data,\n",
    "                                    folds = folds,\n",
    "                                    fold_id=fold_id, \n",
    "                                    wandb_config = config,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed, \n",
    "                                    tokenizer = tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoaders.BuildTrainingLoader(sampler_choice=config.sampling_procedure, num_workers=2, weight_args=['SMILES_Canonical_RDKit','effect','endpoint'])\n",
    "    val_dataloader = DataLoaders.BuildValidationLoader(sampler_choice='SequentialSampler', num_workers=2)\n",
    "    print('Successfully built dataloader')\n",
    "    print(f'SMILES overlap train/validation: {len(set(DataLoaders.train.SMILES_Canonical_RDKit.tolist())&set(DataLoaders.val.SMILES_Canonical_RDKit.tolist()))}')\n",
    "\n",
    "######## MODEL ##################################################################################\n",
    "    # Build the model (consisting of a DNN(-module) and ChemBERTa)\n",
    "    dnn_module = DNN_module(\n",
    "                        one_hot_enc_len=fc1,\n",
    "                        n_hidden_layers=config.n_hidden_layers,\n",
    "                        layer_sizes=GetLayers(config),\n",
    "                        dropout=config.dropout)\n",
    "\n",
    "    model = ecoCAIT(roberta=chemberta, dnn=dnn_module)\n",
    "\n",
    "    model = Modify_architecture(model).FreezeModel(model, config.n_frozen_layers, config.freeze_embedding)\n",
    "    model = Modify_architecture(model).ReinitializeEncoderLayers(model, reinit_n_layers=config.reinit_n_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "######## TRAINING CONFIG ##################################################################################   \n",
    "    # Apply Layer wise Learning Rate Decay\n",
    "    model_parameters = Modify_architecture(model).LLRD(model, init_lr = config.lr)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model_parameters, lr=config.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*config.epochs*len(train_dataloader), num_training_steps=config.epochs*len(train_dataloader))\n",
    "    print('Successfully built optimizer')\n",
    "\n",
    "    if config.loss_fun == 'MSELoss':\n",
    "        loss_fun = nn.MSELoss()\n",
    "    else:\n",
    "        loss_fun = nn.L1Loss()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_loss_norm = np.inf\n",
    "    \n",
    "    batch_num = [0,0]\n",
    "    \n",
    "######## RUN TRAINING ##################################################################################\n",
    "    # Log initial validation loss\n",
    "    avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, -1, global_step-1)\n",
    "\n",
    "    if median_loss < best_val_loss:\n",
    "        best_val_loss = median_loss\n",
    "    \n",
    "    # Initilize arrays in which to store this fold's performance\n",
    "    Best_Validation_Median_Loss = []\n",
    "    Best_Validation_Median_Loss_Normalized = []\n",
    "    Best_Validation_Mean_Loss_Normalized = []\n",
    "\n",
    "    # Run epochs\n",
    "    print(\"\\nRunning epochs...\")\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "\n",
    "        avg_loss, median_loss, total_preds, total_labels, batch_num = train(config, model, train_dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        # Update and log epoch results\n",
    "        if median_loss_norm < best_val_loss_norm:\n",
    "            best_val_loss = median_loss\n",
    "            best_val_loss_norm = median_loss_norm\n",
    "            best_validation_results = val_results\n",
    "            best_validation_mean_norm_loss = avg_loss_norm\n",
    "\n",
    "        wandb.log({'Best Validation Median Loss': best_val_loss,\n",
    "                    'Best Validation Median Loss Normalized': best_val_loss_norm,\n",
    "                    'Best Validation Mean Loss Normalized': best_validation_mean_norm_loss,\n",
    "                    'global_step': global_step})\n",
    "        \n",
    "        Best_Validation_Median_Loss.append(best_val_loss)\n",
    "        Best_Validation_Median_Loss_Normalized.append(best_val_loss_norm)\n",
    "        Best_Validation_Mean_Loss_Normalized.append(best_validation_mean_norm_loss)\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "######## DELETE FOLD PARAMETERS ##################################################################################\n",
    "    del model\n",
    "    del optimizer\n",
    "    del loss_fun\n",
    "    del chemberta\n",
    "    del tokenizer\n",
    "\n",
    "    return Best_Validation_Median_Loss, Best_Validation_Median_Loss_Normalized, Best_Validation_Mean_Loss_Normalized, global_step, best_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:58.430879Z",
     "iopub.status.busy": "2022-05-04T11:40:58.425461Z",
     "iopub.status.idle": "2022-05-04T11:40:59.477198Z",
     "shell.execute_reply": "2022-05-04T11:40:59.476376Z",
     "shell.execute_reply.started": "2022-05-04T11:40:58.430838Z"
    },
    "id": "4BFUym5kRMCq"
   },
   "outputs": [],
   "source": [
    "# function to train the model on epoch\n",
    "def train(args, model, dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    total_loss = 0\n",
    "    total_preds=[]\n",
    "    total_labels=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        # Extract batch samples\n",
    "        batch = [r.to(device) for r in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict batch\n",
    "        preds, _ = model(sent_id, mask, duration, onehot)\n",
    "\n",
    "        # Calculate batch loss\n",
    "        loss = loss_fun(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradient to prevent exploding gradients and update weights\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log batch results\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        total_labels.append(labels)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Training Batch Loss\": loss.item(),\n",
    "            \"Learning Rate\": optimizer.param_groups[0][\"lr\"], \n",
    "            'training batch': batch_num[0]\n",
    "        })\n",
    "        batch_num[0] += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    median_loss = np.median(abs(total_preds - total_labels))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training Loss function\": avg_loss,\n",
    "        \"Training Mean Loss\": np.mean(abs(total_preds - total_labels)), \n",
    "        'training epoch': epoch,\n",
    "        \"Training Median Loss\": np.median(abs(total_preds - total_labels)),\n",
    "        \"Training RMSE Loss\": np.sqrt(np.mean((total_labels-total_preds)**2)),\n",
    "        'global_step': global_step})\n",
    "    \n",
    "    return avg_loss, median_loss, total_preds, total_labels, batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T11:40:59.479019Z",
     "iopub.status.busy": "2022-05-04T11:40:59.478572Z",
     "iopub.status.idle": "2022-05-04T11:40:59.492216Z",
     "shell.execute_reply": "2022-05-04T11:40:59.491432Z",
     "shell.execute_reply.started": "2022-05-04T11:40:59.478936Z"
    },
    "id": "NPhmG5KCRMCq"
   },
   "outputs": [],
   "source": [
    "# function to validate the model on epoch\n",
    "def evaluate(model, dataloader, dataset, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize validation array in which to log results\n",
    "    val_results = dataset.copy()\n",
    "    cls_embeddings = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        # Extract batch samples\n",
    "        batch = [t.to(device) for t in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Predict batch\n",
    "            preds, roberta_output = model(sent_id, mask, duration, onehot)\n",
    "\n",
    "            # Calculate batch loss\n",
    "            loss = loss_fun(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log batch results\n",
    "            cls_embeddings.append(roberta_output.detach().cpu().numpy())\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "        batch_num[1] += 1\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    val_results['CLS_embeddings']  = np.concatenate(cls_embeddings, axis=0).tolist()\n",
    "    val_results['labels'] = total_labels\n",
    "    val_results['preds'] = total_preds\n",
    "    val_results['residuals'] = val_results.labels-val_results.preds\n",
    "    val_results['L1Error'] = abs(total_labels - total_preds)\n",
    "    median_loss = val_results.L1Error.median()\n",
    "    val_results_normalized = CalculateWeightedAverage(val_results)\n",
    "    median_loss_norm = abs(val_results_normalized.residuals).median()\n",
    "    avg_loss_norm = abs(val_results_normalized.residuals).mean()\n",
    "    wandb.log({\n",
    "        \"Validation Loss function\": avg_loss,\n",
    "        \"Validation Mean Loss\": val_results.L1Error.mean(),\n",
    "        \"Validation Median Loss\": median_loss,\n",
    "        \"Validation Loss Normalized\": median_loss_norm,\n",
    "        \"Validation Mean Loss Normalized\": avg_loss_norm,\n",
    "        \"Validation RMSE Loss Normalized\": np.sqrt(((val_results_normalized.labels - val_results_normalized.preds)**2).mean()),\n",
    "        'validation epoch': epoch,\n",
    "        'global_step': global_step\n",
    "        })\n",
    "        \n",
    "    return avg_loss, avg_loss_norm, median_loss, median_loss_norm, total_preds, batch_num, val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(config=None):\n",
    "    from tqdm.notebook import tqdm\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    SetSeed(42)\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "        sweepconfig = wandb.config\n",
    "\n",
    "    ######## DATA ##################################################################################\n",
    "        # Load dataframe\n",
    "        datadir = '../data/development/'\n",
    "        data = pd.read_excel(datadir+'Preprocessed_complete_data.xlsx', sheet_name='dataset')\n",
    "        data, fc1 = GetData(data, sweepconfig)\n",
    "        print('Successfully loaded data')\n",
    "\n",
    "        # Build K-folds based on SMILES (each fold has a unique set of SMILES)\n",
    "        folds = Make_KFolds().Split(data[sweepconfig.smiles_col_name], k_folds=sweepconfig.k_folds, seed=sweepconfig.seed)\n",
    "        print('Successfully built folds')\n",
    "        name = wandb.run.name\n",
    "        \n",
    "        # Initilize arrays in which to store each fold's performance\n",
    "        Avg_Best_Validation_Median_Loss = np.zeros((sweepconfig.k_folds, sweepconfig.epochs))\n",
    "        Avg_Best_Validation_Median_Loss_Normalized = np.zeros((sweepconfig.k_folds, sweepconfig.epochs))\n",
    "        Avg_Best_Validation_Mean_Loss_Normalized = np.zeros((sweepconfig.k_folds, sweepconfig.epochs))\n",
    "\n",
    "        # Run K-fold Cross validation\n",
    "        print(f'\\n Running {sweepconfig.k_folds} folds')\n",
    "        global_step = 0\n",
    "\n",
    "        for fold_id in tqdm(range(1, sweepconfig.k_folds+1, 1)):\n",
    "            # Run one fold\n",
    "            Best_Validation_Median_Loss, Best_Validation_Median_Loss_Normalized, Best_Validation_Mean_Loss_Normalized, global_step, best_validation_results = RunTrainingEpochs(data, folds, fold_id, sweepconfig, fc1, global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # Log results\n",
    "            if sweepconfig.save_results == True:\n",
    "                wandb.log({f\"Best Validation Results {fold_id}\": wandb.Table(dataframe=best_validation_results)})\n",
    "            Avg_Best_Validation_Median_Loss[fold_id-1,:] = Best_Validation_Median_Loss\n",
    "            Avg_Best_Validation_Median_Loss_Normalized[fold_id-1,:] = Best_Validation_Median_Loss_Normalized\n",
    "            Avg_Best_Validation_Mean_Loss_Normalized[fold_id-1,:] = Best_Validation_Mean_Loss_Normalized\n",
    "\n",
    "        Avg_Best_Validation_Median_Loss.mean(axis=0)\n",
    "        Avg_Best_Validation_Median_Loss_Normalized.mean(axis=0)\n",
    "\n",
    "        # Log the sweep metric as the average performance\n",
    "        for i in range(sweepconfig.epochs):\n",
    "            wandb.log({\n",
    "                'Avg Best Validation Median Loss Normalized': Avg_Best_Validation_Median_Loss_Normalized.mean(axis=0)[i],\n",
    "                'Avg Best Validation Median Loss Normalized std': np.std(Avg_Best_Validation_Median_Loss_Normalized, axis=0)[i],\n",
    "                'Avg Best Validation Median Loss': Avg_Best_Validation_Median_Loss.mean(axis=0)[i],\n",
    "                'Avg Best Validation Median Loss std': np.std(Avg_Best_Validation_Median_Loss, axis=0)[i],\n",
    "                'Avg Best Validation Mean Loss Normalized': Avg_Best_Validation_Mean_Loss_Normalized.mean(axis=0)[i],\n",
    "                'epoch': i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s5W5wHbRMCr"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run wandb agent (runs the script)\n",
    "wandb.agent(f'{ENITITYNAME}/{PROJECTNAME}/{SWEEPID}', trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fishbait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1514c81f3705f7107573a782eeac851e32c3c00fc4e7dabdb8a1c6e8ef21406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
