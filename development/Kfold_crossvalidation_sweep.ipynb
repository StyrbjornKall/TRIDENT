{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and launch ten times repeated 10-fold cross-validation on multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T12:20:06.573146Z",
     "iopub.status.busy": "2022-04-07T12:20:06.572791Z",
     "iopub.status.idle": "2022-04-07T12:20:06.594684Z",
     "shell.execute_reply": "2022-04-07T12:20:06.594057Z",
     "shell.execute_reply.started": "2022-04-07T12:20:06.573114Z"
    },
    "id": "MjtASeFVRMCW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skall\\Anaconda3\\envs\\fishbait\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\skall\\Anaconda3\\envs\\fishbait\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from development_utils.preprocessing.Get_data_for_model import PreprocessData\n",
    "from development_utils.training.Build_Pytorch_Dataset_and_DataLoader import BuildDataLoader_KFold\n",
    "from development_utils.training.Build_Pytorch_model import fishbAIT, DNN_module, GPUinfo\n",
    "from development_utils.training.PerformanceCalculations import CalculateWeightedAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.327439Z",
     "iopub.status.busy": "2022-04-01T05:46:31.326981Z",
     "iopub.status.idle": "2022-04-01T05:46:31.394170Z",
     "shell.execute_reply": "2022-04-01T05:46:31.393370Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.327400Z"
    },
    "id": "d60ceda8",
    "outputId": "93f57e48-3fb9-49f9-f4b0-48d51e958abd",
    "papermill": {
     "duration": 0.087271,
     "end_time": "2022-03-04T15:15:20.579086",
     "exception": false,
     "start_time": "2022-03-04T15:15:20.491815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUinfo(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITYNAME = 'ecotoxformer'\n",
    "PROJECTNAME = 'testing_github'\n",
    "SWEEPID = 'g9ji6hl9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T05:46:31.395755Z",
     "iopub.status.busy": "2022-04-01T05:46:31.395495Z",
     "iopub.status.idle": "2022-04-01T05:46:37.015133Z",
     "shell.execute_reply": "2022-04-01T05:46:37.014459Z",
     "shell.execute_reply.started": "2022-04-01T05:46:31.395718Z"
    },
    "id": "826933f3",
    "outputId": "3657c92a-871e-4cd8-d2c6-c1b913230444",
    "papermill": {
     "duration": 1.576782,
     "end_time": "2022-03-04T15:15:22.19011",
     "exception": true,
     "start_time": "2022-03-04T15:15:20.613328",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(data, config):\n",
    "    processor = PreprocessData(dataframe=data)\n",
    "\n",
    "    processor.FilterData(\n",
    "        concentration_thresh=config.conc_thresh,\n",
    "        endpoint=config.endpoints,\n",
    "        effect=config.effects,\n",
    "        species_groups=config.species_groups,\n",
    "        log_data=True,\n",
    "        concentration_sign=config.concentration_sign)\n",
    "\n",
    "    processor.GetPubchemCID()\n",
    "    processor.GetMetadata(['cmpdname'])\n",
    "    processor.GetCanonicalSMILES()\n",
    "    processor.GetOneHotEndpoint(config.endpoints)\n",
    "    processor.GetOneHotEffect(config.effects)\n",
    "    processor.ConcatenateOneHotEnc()\n",
    "\n",
    "    data = processor.dataframe\n",
    "    fc1 = len(data.OneHotEnc_concatenated.iloc[0])\n",
    "    \n",
    "    return data, fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetSeed(seed):\n",
    "    torch.manual_seed(seed) # pytorch random seed\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLayers(config):\n",
    "    if config.n_hidden_layers == 1:\n",
    "        return [config.layer_1]\n",
    "    elif config.n_hidden_layers == 2:\n",
    "        return [config.layer_1, config.layer_2]\n",
    "    elif config.n_hidden_layers == 3:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3]\n",
    "    elif config.n_hidden_layers == 4:\n",
    "        return [config.layer_1, config.layer_2, config.layer_3, config.layer_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunTrainingEpochs(data, folds, fold_id, config, fc1, global_step):\n",
    "    \n",
    "    chemberta = AutoModel.from_pretrained(config.base_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "    DataLoaders = BuildDataLoader_KFold(\n",
    "                                    df = data,\n",
    "                                    folds = folds,\n",
    "                                    fold_id=fold_id, \n",
    "                                    variables = config.inputs,\n",
    "                                    label = config.label, \n",
    "                                    batch_size = config.batch_size, \n",
    "                                    max_length = config.max_token_length, \n",
    "                                    seed = config.seed, \n",
    "                                    tokenizer = tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoaders.BuildTrainingLoader(sampler_choice=config.sampling_procedure, num_workers=2, weight_args=['SMILES_Canonical_RDKit','COMBINED_effect','COMBINED_endpoint'])\n",
    "    val_dataloader = DataLoaders.BuildValidationLoader(sampler_choice='SequentialSampler', num_workers=2)\n",
    "    print('Successfully built dataloader')\n",
    "    print(f'SMILES overlap train/validation: {len(set(DataLoaders.train.SMILES_Canonical_RDKit.tolist())&set(DataLoaders.val.SMILES_Canonical_RDKit.tolist()))}')\n",
    "    \n",
    "    wandb.log({\"Training df\": wandb.Table(dataframe=DataLoaders.train)})\n",
    "######## MODEL ##################################################################################\n",
    "    dnn_module = DNN_module(\n",
    "                        one_hot_enc_len=fc1,\n",
    "                        n_hidden_layers=config.n_hidden_layers,\n",
    "                        layer_sizes=GetLayers(config),\n",
    "                        dropout=config.dropout,\n",
    "                        activation='ReLU')\n",
    "\n",
    "    model = fishbAIT(roberta=chemberta, dnn=dnn_module)\n",
    "\n",
    "    model = Modify_architecture(model).FreezeModel(model, config.n_frozen_layers, config.freeze_embedding)\n",
    "    model = Modify_architecture(model).ReinitializeEncoderLayers(model, reinit_n_layers=config.reinit_n_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "######## TRAINING CONFIG ##################################################################################        \n",
    "    model_parameters = Modify_architecture(model).LLRD(model, init_lr = config.lr)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model_parameters, lr=config.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*config.epochs*len(train_dataloader), num_training_steps=config.epochs*len(train_dataloader))\n",
    "    print('Successfully built optimizer')\n",
    "\n",
    "    if config.loss_fun == 'MSELoss':\n",
    "        loss_fun = nn.MSELoss()\n",
    "    else:\n",
    "        loss_fun = nn.L1Loss()\n",
    "\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_loss_norm = np.inf\n",
    "    \n",
    "    batch_num = [0,0]\n",
    "\n",
    "    if len(config.endpoints) == 1:\n",
    "        endpoint='EC50'\n",
    "    elif len(config.endpoints) == 2:\n",
    "        endpoint='EC10'\n",
    "    elif len(config.endpoints) == 3:\n",
    "        endpoint='EC50EC10'\n",
    "\n",
    "    save_name = f'/mimer/NOBACKUP/groups/snic2022-22-552/skall/100_foldCV_models_and_data_{endpoint}/fold{config.fold_id}_seed{config.seed}_model'\n",
    "    def save_ckp(model, checkpoint_dir):\n",
    "        torch.save(model.dnn.state_dict(), checkpoint_dir+'_dnn_saved_weights.pt')\n",
    "        torch.save(model.roberta.state_dict(), checkpoint_dir+'_roberta_saved_weights.pt')\n",
    "\n",
    "######## RUN TRAINING ##################################################################################\n",
    "\n",
    "    avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, -1, global_step-1)\n",
    "\n",
    "    if median_loss < best_val_loss:\n",
    "        best_val_loss = median_loss\n",
    "    \n",
    "    print(\"\\nRunning epochs...\")\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "\n",
    "        avg_loss, median_loss, total_preds, total_labels, batch_num = train(config, model, train_dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        avg_loss, avg_loss_norm, median_loss, median_loss_norm, _, batch_num, val_results = evaluate(model, val_dataloader, DataLoaders.val, loss_fun, batch_num, epoch, global_step)\n",
    "        \n",
    "        if median_loss_norm < best_val_loss_norm:\n",
    "            best_val_loss = median_loss\n",
    "            best_val_loss_norm = median_loss_norm\n",
    "            best_validation_results = val_results\n",
    "            best_validation_mean_norm_loss = avg_loss_norm\n",
    "            if config.save_model == True:\n",
    "                save_ckp(model, save_name)\n",
    "\n",
    "        wandb.log({'Best Validation Median Loss': best_val_loss,\n",
    "                    'Best Validation Median Loss Normalized': best_val_loss_norm,\n",
    "                    'Best Validation Mean Loss Normalized': best_validation_mean_norm_loss,\n",
    "                    'global_step': global_step})\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "\n",
    "######## SAVE RESULTS ##################################################################################\n",
    "    if config.save_results == True:\n",
    "        DataLoaders.train.to_csv(save_name+'_trainingdata.csv', index=False)    \n",
    "        DataLoaders.val.to_csv(save_name+'_validationdata.csv', index=False)   \n",
    "        wandb.log({\"Best Validation Results\": wandb.Table(dataframe=best_validation_results),\n",
    "                    \"Training data\": wandb.Table(dataframe=DataLoaders.train),\n",
    "                    \"Validation data\": wandb.Table(dataframe=DataLoaders.val)})\n",
    "\n",
    "    if config.save_final_epoch == True:\n",
    "        wandb.log({\"Validation Results Final Epoch\": wandb.Table(dataframe=val_results)})\n",
    "        \n",
    "######## DELETE FOLD PARAMETERS ##################################################################################\n",
    "    del model\n",
    "    del optimizer\n",
    "    del loss_fun\n",
    "    del chemberta\n",
    "    del tokenizer\n",
    "\n",
    "    return best_val_loss, best_val_loss_norm, best_validation_mean_norm_loss, global_step, best_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(args, model, dataloader, optimizer, scheduler, loss_fun, batch_num, epoch, global_step):\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    total_loss = 0\n",
    "    total_preds=[]\n",
    "    total_labels=[]\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = [r.to(device) for r in batch.values()]\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds, _ = model(sent_id, mask, duration, onehot)\n",
    "        loss = loss_fun(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        total_labels.append(labels)\n",
    "        \n",
    "        \n",
    "        wandb.log({\n",
    "            \"Training Batch Loss\": loss.item(),\n",
    "            \"Learning Rate\": optimizer.param_groups[0][\"lr\"], \n",
    "            'training batch': batch_num[0]\n",
    "        })\n",
    "        batch_num[0] += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    median_loss = np.median(abs(total_preds - total_labels))\n",
    "\n",
    "    wandb.log({\n",
    "        \"Training Loss function\": avg_loss,\n",
    "        \"Training Mean Loss\": np.mean(abs(total_preds - total_labels)), \n",
    "        'training epoch': epoch,\n",
    "        \"Training Median Loss\": np.median(abs(total_preds - total_labels)),\n",
    "        \"Training RMSE Loss\": np.sqrt(np.mean((total_labels-total_preds)**2)),\n",
    "        'global_step': global_step})\n",
    "    \n",
    "    return avg_loss, median_loss, total_preds, total_labels, batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, dataloader, dataset, loss_fun, batch_num, epoch, global_step):\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    val_results = dataset.copy()\n",
    "    cls_embeddings = []\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = [t.to(device) for t in batch.values()]\n",
    "\n",
    "        sent_id, mask, duration, onehot, labels = batch\n",
    "        with torch.no_grad():\n",
    "            preds, roberta_output = model(sent_id, mask, duration, onehot)\n",
    "            loss = loss_fun(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            cls_embeddings.append(roberta_output.detach().cpu().numpy())\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "        batch_num[1] += 1\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    total_labels  = np.concatenate(total_labels, axis=0)\n",
    "    val_results['CLS_embeddings']  = np.concatenate(cls_embeddings, axis=0).tolist()\n",
    "    val_results['labels'] = total_labels\n",
    "    val_results['preds'] = total_preds\n",
    "    val_results['residuals'] = val_results.labels-val_results.preds\n",
    "    val_results['L1Error'] = abs(total_labels - total_preds)\n",
    "    median_loss = val_results.L1Error.median()\n",
    "    val_results_normalized = CalculateWeightedAverage(val_results)\n",
    "    median_loss_norm = abs(val_results_normalized.residuals).median()\n",
    "    avg_loss_norm = abs(val_results_normalized.residuals).mean()\n",
    "    wandb.log({\n",
    "        \"Validation Loss function\": avg_loss,\n",
    "        \"Validation Mean Loss\": val_results.L1Error.mean(),\n",
    "        \"Validation Median Loss\": median_loss,\n",
    "        \"Validation Loss Normalized\": median_loss_norm,\n",
    "        \"Validation Mean Loss Normalized\": avg_loss_norm,\n",
    "        \"Validation RMSE Loss Normalized\": np.sqrt(((val_results_normalized.labels - val_results_normalized.preds)**2).mean()),\n",
    "        'validation epoch': epoch,\n",
    "        'global_step': global_step\n",
    "        })\n",
    "        \n",
    "    return avg_loss, avg_loss_norm, median_loss, median_loss_norm, total_preds, batch_num, val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(config=None):\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    SetSeed(42)\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        # If called by wandb.agent, as below, this config will be set by Sweep Controller\n",
    "        sweepconfig = wandb.config\n",
    "\n",
    "        datadir = '../data/development/'\n",
    "\n",
    "    ######## DATA ##################################################################################\n",
    "        data = pd.read_excel(datadir+'Preprocessed_complete_data.xlsx')\n",
    "        data, fc1 = GetData(data, sweepconfig)\n",
    "        print('Successfully loaded data')\n",
    "\n",
    "        folds = Make_KFolds().Split(data[sweepconfig.smiles_col_name], k_folds=sweepconfig.k_folds, seed=sweepconfig.seed)\n",
    "        print('Successfully built folds')\n",
    "        name = wandb.run.name\n",
    "        \n",
    "        print(f'\\n Running fold {sweepconfig.fold_id} using seed {sweepconfig.seed}')\n",
    "        global_step = 0\n",
    "\n",
    "        _, _, _, global_step, _ = RunTrainingEpochs(data, folds, sweepconfig.fold_id, sweepconfig, fc1, global_step)\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s5W5wHbRMCr"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dlt7mng8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: ChemBERTa+DNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model: seyonec/PubChem10M_SMILES_BPE_450k\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconc_thresh: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconcentration_sign: =\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: large\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffects: ['MOR', 'DVP', 'POP', 'MPH', 'ITX', 'REP', 'GRO']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tendpoints: ['EC10', 'NOEC']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfold_id: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_embedding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinputs: ['SMILES_Canonical_RDKit', 'COMBINED_Duration_Value', 'OneHotEnc_concatenated']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk_folds: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlabel: COMBINED_mgperL\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_1: 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_2: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_3: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fun: L1Loss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_token_length: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_frozen_layers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treinit_n_layers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling_procedure: WRS_sqrt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_final_epoch: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_model: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_results: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsmiles_col_name: SMILES_Canonical_RDKit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tspecies_classes: []\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tspecies_groups: ['fish']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_cls: True\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstyrbjornkall\u001b[0m (\u001b[33mecotoxformer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\skall\\OneDrive - Chalmers\\Documents\\git_repo\\development\\wandb\\run-20221201_162912-dlt7mng8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ecotoxformer/testing_github/runs/dlt7mng8\" target=\"_blank\">electric-sweep-1</a></strong> to <a href=\"https://wandb.ai/ecotoxformer/testing_github\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ecotoxformer/testing_github/sweeps/g9ji6hl9\" target=\"_blank\">https://wandb.ai/ecotoxformer/testing_github/sweeps/g9ji6hl9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-sweep-1</strong>: <a href=\"https://wandb.ai/ecotoxformer/testing_github/runs/dlt7mng8\" target=\"_blank\">https://wandb.ai/ecotoxformer/testing_github/runs/dlt7mng8</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221201_162912-dlt7mng8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run dlt7mng8 errored: TypeError(\"read_excel() got an unexpected keyword argument 'encoding'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run dlt7mng8 errored: TypeError(\"read_excel() got an unexpected keyword argument 'encoding'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0wjp9xky with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: ChemBERTa+DNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_model: seyonec/PubChem10M_SMILES_BPE_450k\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconc_thresh: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconcentration_sign: =\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: large\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffects: ['MOR', 'DVP', 'POP', 'MPH', 'ITX', 'REP', 'GRO']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tendpoints: ['EC10', 'NOEC']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfold_id: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_embedding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinputs: ['SMILES_Canonical_RDKit', 'COMBINED_Duration_Value', 'OneHotEnc_concatenated']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tk_folds: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlabel: COMBINED_mgperL\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_1: 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_2: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_3: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fun: L1Loss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_token_length: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_frozen_layers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treinit_n_layers: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampling_procedure: WRS_sqrt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_final_epoch: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_model: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_results: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsmiles_col_name: SMILES_Canonical_RDKit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tspecies_classes: []\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tspecies_groups: ['fish']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_cls: True\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\skall\\OneDrive - Chalmers\\Documents\\git_repo\\development\\wandb\\run-20221201_162932-0wjp9xky</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ecotoxformer/testing_github/runs/0wjp9xky\" target=\"_blank\">cerulean-sweep-2</a></strong> to <a href=\"https://wandb.ai/ecotoxformer/testing_github\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ecotoxformer/testing_github/sweeps/g9ji6hl9\" target=\"_blank\">https://wandb.ai/ecotoxformer/testing_github/sweeps/g9ji6hl9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = SWEEPID\n",
    "wandb.agent(f'{ENTITYNAME}/{PROJECTNAME}/'+sweep_id, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fishbait')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1514c81f3705f7107573a782eeac851e32c3c00fc4e7dabdb8a1c6e8ef21406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
