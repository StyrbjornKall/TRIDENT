{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script showcases the different models available in TRIDENT and how to use them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.insert(1, '/cephyr/users/skall/Alvis/Ecotoxformer/inference_scripts_for_website_2/TRIDENT/tutorials/inference_utils/')\n",
    "sys.path.insert(1, '/cephyr/users/skall/Alvis/Ecotoxformer/inference_scripts_for_website_2/TRIDENT/development/development_utils/')\n",
    "from inference_utils.TRIDENT_for_inference import TRIDENT_for_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model version and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ZENODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/cephyr/users/skall/Alvis/TRIDENT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for MODEL_ENDPOINT in ['EC50', 'EC10', 'EC50EC10']:\n",
    "    for SPECIES in tqdm(['fish','invertebrates','algae']):\n",
    "\n",
    "        MODEL_VERSION = f'{MODEL_ENDPOINT}_{SPECIES}'\n",
    "        print(f'Running predictions on: {MODEL_ENDPOINT}_{SPECIES}\\n')\n",
    "        print('\\t Loading model...\\n')\n",
    "        trident = TRIDENT_for_inference(model_version=MODEL_VERSION, path_to_model_weights=root+'TRIDENT/')\n",
    "        trident.load_fine_tuned_model()\n",
    "        print('\\t Loading data...\\n')\n",
    "        data = pd.read_excel(root+'data/Preprocessed_complete_data.xlsx', sheet_name=f'{MODEL_ENDPOINT}_{SPECIES}')\n",
    "        data['species_group'] = data.species_group.replace('crustaceans', 'invertebrates')\n",
    "        \n",
    "        print('\\t Predicting...\\n')\n",
    "        results = trident.predict_toxicity(SMILES = data['SMILES'].tolist(), exposure_duration=data['Duration_Value'].tolist(), endpoint=data['endpoint'].tolist(), effect=data['effect'], return_cls_embeddings=False)\n",
    "\n",
    "        results['exposure_duration'] = 10**results.exposure_duration\n",
    "\n",
    "        data = data.sort_values(by=['SMILES','Duration_Value','effect','endpoint'])\n",
    "        results = results.sort_values(by=['SMILES','exposure_duration','effect','endpoint'])\n",
    "\n",
    "        data = pd.concat([data,results[['predictions log10(mg/L)','predictions (mg/L)']]], axis=1)\n",
    "\n",
    "        data = data.sort_index()\n",
    "        print(f'Saving...\\n')\n",
    "        data.to_csv(root+f'data/Preprocessed_complete_data_pred_{MODEL_ENDPOINT}_{SPECIES}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_groups = ['fish', 'algae', 'invertebrates']\n",
    "models = ['EC50EC10', 'EC50', 'EC10']\n",
    "effectordering = {\n",
    "            'EC50_algae': {'POP':'POP'},\n",
    "            'EC10_algae': {'POP':'POP'},\n",
    "            'EC50EC10_algae': {'POP':'POP'}, \n",
    "            'EC50_invertebrates': {'MOR':'MOR','ITX':'ITX'},\n",
    "            'EC10_invertebrates': {'MOR':'MOR','DVP':'DVP','ITX':'ITX', 'REP': 'REP', 'MPH': 'MPH', 'POP': 'POP'} ,\n",
    "            'EC50EC10_invertebrates': {'MOR':'MOR','DVP':'DVP','ITX':'ITX', 'REP': 'REP', 'MPH': 'MPH', 'POP': 'POP'} ,\n",
    "            'EC50_fish': {'MOR':'MOR'},\n",
    "            'EC10_fish': {'MOR':'MOR','DVP':'DVP','ITX':'ITX', 'REP': 'REP', 'MPH': 'MPH', 'POP': 'POP','GRO': 'GRO'} ,\n",
    "            'EC50EC10_fish': {'MOR':'MOR','DVP':'DVP','ITX':'ITX', 'REP': 'REP', 'MPH': 'MPH', 'POP': 'POP','GRO': 'GRO'} \n",
    "            }\n",
    "\n",
    "endpointordering = {\n",
    "            'EC50_algae': {'EC50':'EC50'},\n",
    "            'EC10_algae': {'EC10':'EC10'},\n",
    "            'EC50EC10_algae': {'EC50':'EC50', 'EC10': 'EC10'}, \n",
    "            'EC50_invertebrates': {'EC50':'EC50'},\n",
    "            'EC10_invertebrates': {'EC10':'EC10'},\n",
    "            'EC50EC10_invertebrates': {'EC50':'EC50', 'EC10': 'EC10'},\n",
    "            'EC50_fish': {'EC50':'EC50'},\n",
    "            'EC10_fish': {'EC10':'EC10'},\n",
    "            'EC50EC10_fish': {'EC50':'EC50', 'EC10': 'EC10'} \n",
    "            }\n",
    "\n",
    "default_durations = {\n",
    "    'algae': 72,\n",
    "    'fish': 96,\n",
    "    'invertebrates': 48\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(root+'data/development/Preprocessed_complete_data.xlsx', sheet_name='dataset')\n",
    "\n",
    "raw_data['species_group'] = raw_data.species_group.replace('crustaceans', 'invertebrates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a878ef49674218af4282cd282d5fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342b75bf9c334a67bec30e09e58b6ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a9d824fc8242dca3c289b4852d6ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90764f164dd1422b92bb707c37bb566f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a383929d7d43ccbcaf5092cf2009a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41641d0af9f346a4b2fdb2c050c8fde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26cc2bf36cd4857a6ce8195c11a4ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df0f83909654034b7a97583e265a264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed NOEC *EC10* in 0 positions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:01<00:26,  1.07s/it]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:19,  1.26it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:02<00:16,  1.40it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.47it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.50it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:04<00:13,  1.53it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:12,  1.57it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.58it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:06<00:10,  1.59it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.59it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.59it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:08,  1.60it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.60it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:09<00:07,  1.60it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:06,  1.59it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.59it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:11<00:05,  1.59it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.58it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.58it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.58it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.58it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.58it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.57it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.62it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:16<00:00,  1.61it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.59it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9eeeba8c1d43a9bce8dcc8f86d8809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bb8e4f4f0d40bfb6299b660345e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d95cac69ed4afb80ccf179816b28bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75905c556211481e9d0ee89eedfd7fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cee7a4271c43f6bcf957f0af38787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f67dcb85b7e4c538e3a91a4dc605cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3184a5331a042be81ac3efcd9985ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not return onehotencoding for Endpoint. Why? You specified only one Endpoint or you specified NOEC and EC10 which are coded to be the same endpoint.\n",
      "Did not return onehotencoding for Effect. Why? You specified only one Effect.\n",
      "Will use input 0 to network due to no Onehotencodings being present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.54it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:14,  1.57it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.57it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.58it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:12,  1.58it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:11,  1.60it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.59it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.59it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.59it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:06<00:09,  1.58it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:08,  1.59it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.58it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:08<00:07,  1.58it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.57it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.57it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:10<00:05,  1.57it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.57it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.57it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.56it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.57it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.56it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.56it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.59it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:15<00:00,  1.58it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.61it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62031e4811114835b6ccfaeb38e81a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dca189730247ffa7b6824214984fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abcac3d113940afb4bc837c97a09c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f57f2d6d6bf4ba9b51aa13e6f6c2d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351dae74786b4540894546c351d2b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd8e7cf8d7a45cbb93fea3bcdd65f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d9e89035cd459c91bb7f78378a29c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed NOEC *EC10* in 0 positions\n",
      "Did not return onehotencoding for Endpoint. Why? You specified only one Endpoint or you specified NOEC and EC10 which are coded to be the same endpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.55it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:14,  1.55it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.56it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.57it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:12,  1.57it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:11,  1.60it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.59it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.58it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.58it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.57it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:08,  1.57it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.57it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:08<00:07,  1.57it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.57it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.56it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:10<00:05,  1.56it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.56it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.56it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.56it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.56it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.56it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.55it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.59it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:15<00:00,  1.57it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.60it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02617577dd9d4f4d81480fcbbef2b3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82585302b29f4d7f87b6ddbfe055b178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7226be65a4f14f53b64ac59d4db1e410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e248b7954e4ac3bcd0c5ece91d49ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e3a5a421bc40e7ab2542bc623353a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9617728fb37c45e187cf41cc71186d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d02dccdfacd42c6b5442e05f8763e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed NOEC *EC10* in 0 positions\n",
      "Did not return onehotencoding for Effect. Why? You specified only one Effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.52it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:14,  1.54it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.55it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.54it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:12,  1.54it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:12,  1.57it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.56it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.56it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.55it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.55it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:08,  1.56it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.55it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:09<00:07,  1.56it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.55it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.55it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:10<00:05,  1.55it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.55it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.56it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.56it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.55it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.55it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.55it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.59it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:16<00:00,  1.57it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.59it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973fa56f24864af7bb6342daf4ca5da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6a4fe64790464d8fdf86b128bc9c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e96c76c34794405b8ecf53a57c18cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14297f5cd6dd498ca499e60cce580804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facf480407ff4b148161094d2c297d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05efede723d45abbaa3aa1e940b4b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b942fdc0da854ccdba14b451041b347c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not return onehotencoding for Endpoint. Why? You specified only one Endpoint or you specified NOEC and EC10 which are coded to be the same endpoint.\n",
      "Did not return onehotencoding for Effect. Why? You specified only one Effect.\n",
      "Will use input 0 to network due to no Onehotencodings being present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.53it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:14,  1.55it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.55it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.55it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:12,  1.55it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:12,  1.57it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.56it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.56it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.56it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.55it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:08,  1.56it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.55it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:09<00:07,  1.56it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.55it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.51it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:11<00:05,  1.52it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.53it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.53it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.54it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.54it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.54it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.53it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.57it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:16<00:00,  1.56it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.58it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8884387cc4974a48882685afd0730aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ddf430af7498eb218c6642b34dccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0fd3aa854d4751aa01a933310ecad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e4b2bde5f24a1cb0b21c9fc46642c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093b86762958453bba0be110fa7c578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dcf3a3f49343ccbf659aa076b6e4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ade55366b624847872d7fc41abc313d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed NOEC *EC10* in 0 positions\n",
      "Did not return onehotencoding for Endpoint. Why? You specified only one Endpoint or you specified NOEC and EC10 which are coded to be the same endpoint.\n",
      "Did not return onehotencoding for Effect. Why? You specified only one Effect.\n",
      "Will use input 0 to network due to no Onehotencodings being present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.40it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.53it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:14,  1.54it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.54it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.55it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:12,  1.55it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:12,  1.57it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.56it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.56it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.56it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.55it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:09,  1.55it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.55it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:09<00:07,  1.55it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.54it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.54it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:11<00:05,  1.54it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.54it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.54it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.54it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.54it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.54it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.54it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.58it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:16<00:00,  1.58it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.58it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cdc095c09246e1b6723c7a3d1325f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548a81bf3c44470ca56a1f05a063636c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893f38e31ea74a888a796ef68fd2a3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2f712ad31c49e3b88371402ce4556a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3d570638e348d59eabaf88d7336ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc37993f4864dd5ab1351a61d223399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dd8b45f46e45a6a4889a59c923f57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed NOEC *EC10* in 0 positions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.53it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:14,  1.54it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.54it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.54it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:12,  1.54it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:12,  1.57it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.56it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.56it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.56it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.55it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:09,  1.55it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.55it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:09<00:07,  1.55it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.54it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.54it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:11<00:05,  1.53it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.53it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.53it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:12<00:03,  1.53it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.53it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.53it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:14<00:01,  1.53it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.57it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:16<00:00,  1.55it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.58it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b134816a7ff548d580f736ed5b88e6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7a31bec8aa4362a1f7dec1b8aeab5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86979ba7fb845e3a3e41c6259376aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72526fd53c914ef484b2cf2a98f6872c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/145k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efc5ec33851489e92280b6d1799d5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/99.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088d490fd77d4db2be4ce089694437a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cff8f8a73944bc89bb13d3e51e65ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not return onehotencoding for Endpoint. Why? You specified only one Endpoint or you specified NOEC and EC10 which are coded to be the same endpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/26 [00:00<00:17,  1.40it/s]\u001b[A\n",
      "  8%|▊         | 2/26 [00:01<00:15,  1.51it/s]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:01<00:15,  1.53it/s]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:02<00:14,  1.53it/s]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:03<00:13,  1.53it/s]\u001b[A\n",
      " 23%|██▎       | 6/26 [00:03<00:13,  1.52it/s]\u001b[A\n",
      " 27%|██▋       | 7/26 [00:04<00:12,  1.54it/s]\u001b[A\n",
      " 31%|███       | 8/26 [00:05<00:11,  1.54it/s]\u001b[A\n",
      " 35%|███▍      | 9/26 [00:05<00:10,  1.55it/s]\u001b[A\n",
      " 38%|███▊      | 10/26 [00:06<00:10,  1.55it/s]\u001b[A\n",
      " 42%|████▏     | 11/26 [00:07<00:09,  1.54it/s]\u001b[A\n",
      " 46%|████▌     | 12/26 [00:07<00:09,  1.54it/s]\u001b[A\n",
      " 50%|█████     | 13/26 [00:08<00:08,  1.54it/s]\u001b[A\n",
      " 54%|█████▍    | 14/26 [00:09<00:07,  1.54it/s]\u001b[A\n",
      " 58%|█████▊    | 15/26 [00:09<00:07,  1.54it/s]\u001b[A\n",
      " 62%|██████▏   | 16/26 [00:10<00:06,  1.54it/s]\u001b[A\n",
      " 65%|██████▌   | 17/26 [00:11<00:05,  1.53it/s]\u001b[A\n",
      " 69%|██████▉   | 18/26 [00:11<00:05,  1.53it/s]\u001b[A\n",
      " 73%|███████▎  | 19/26 [00:12<00:04,  1.53it/s]\u001b[A\n",
      " 77%|███████▋  | 20/26 [00:13<00:03,  1.53it/s]\u001b[A\n",
      " 81%|████████  | 21/26 [00:13<00:03,  1.53it/s]\u001b[A\n",
      " 85%|████████▍ | 22/26 [00:14<00:02,  1.53it/s]\u001b[A\n",
      " 88%|████████▊ | 23/26 [00:15<00:01,  1.53it/s]\u001b[A\n",
      " 92%|█████████▏| 24/26 [00:15<00:01,  1.57it/s]\u001b[A\n",
      " 96%|█████████▌| 25/26 [00:16<00:00,  1.55it/s]\u001b[A\n",
      "100%|██████████| 26/26 [00:16<00:00,  1.57it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "SMILES_COLUMN_NAME = 'SMILES_Canonical_RDKit'\n",
    "for SPECIES in tqdm(species_groups):\n",
    "    for model in models:\n",
    "        cls_dict = {}\n",
    "        MODEL_VERSION = f'{model}_{SPECIES}'\n",
    "        EXPOSURE_DURATION = default_durations[SPECIES]\n",
    "        PREDICTION_ENDPOINT = list(endpointordering[MODEL_VERSION].keys())[0]\n",
    "        PREDICTION_EFFECT = list(effectordering[MODEL_VERSION].keys())[0]\n",
    "        trident = TRIDENT_for_inference(model_version=MODEL_VERSION, path_to_model_weights=root+'TRIDENT/')\n",
    "        trident.load_fine_tuned_model()\n",
    "        data = raw_data.copy()\n",
    "        \n",
    "        data = data.drop_duplicates(subset=['SMILES_Canonical_RDKit']).dropna(subset=['SMILES_Canonical_RDKit'])\n",
    "\n",
    "        results = trident.predict_toxicity(SMILES = data[SMILES_COLUMN_NAME].tolist(), exposure_duration=EXPOSURE_DURATION, endpoint=[PREDICTION_ENDPOINT]*len(data), effect=[PREDICTION_EFFECT]*len(data), return_cls_embeddings=True)\n",
    "        results.reset_index(drop=True, inplace=True)\n",
    "        results['CLS_embeddings'] = results['CLS_embeddings'].apply(lambda x: np.asarray(x, dtype=np.float32))\n",
    "        results = results[['SMILES_Canonical_RDKit','CLS_embeddings']]        \n",
    "        results.to_pickle(root+f'data/tutorials/predictions/{MODEL_VERSION}_CLS_embeddings.pkl.zip', compression='zip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES_COLUMN_NAME = 'SMILES_Canonical_RDKit'\n",
    "\n",
    "combined_results = raw_data.drop_duplicates(subset=['SMILES_Canonical_RDKit']).dropna(subset=['SMILES_Canonical_RDKit'])[[SMILES_COLUMN_NAME]]\n",
    "\n",
    "for SPECIES in tqdm(species_groups):\n",
    "    \n",
    "    for model in models:\n",
    "        MODEL_VERSION = f'{model}_{SPECIES}'\n",
    "        trident = TRIDENT_for_inference(model_version=MODEL_VERSION, path_to_model_weights=root+'TRIDENT/', device='cuda')\n",
    "        trident.load_fine_tuned_model()\n",
    "        data = raw_data.copy()\n",
    "        \n",
    "        for PREDICTION_ENDPOINT in endpointordering[MODEL_VERSION]:\n",
    "            for PREDICTION_EFFECT in effectordering[MODEL_VERSION]:\n",
    "                print(MODEL_VERSION, PREDICTION_ENDPOINT, PREDICTION_EFFECT)\n",
    "                try:\n",
    "                    EXPOSURE_DURATION = data[(data.species_group==SPECIES) & (data.endpoint==PREDICTION_ENDPOINT) & (data.effect==PREDICTION_EFFECT)].Duration_Value.value_counts().index[0]\n",
    "                except:\n",
    "                    EXPOSURE_DURATION = default_durations[SPECIES]\n",
    "                data = combined_results[[SMILES_COLUMN_NAME]]\n",
    "\n",
    "                results = trident.predict_toxicity(SMILES = data[SMILES_COLUMN_NAME].tolist(), exposure_duration=EXPOSURE_DURATION, endpoint=[PREDICTION_ENDPOINT]*len(data), effect=[PREDICTION_EFFECT]*len(data), return_cls_embeddings=True)\n",
    "                results.reset_index(drop=True, inplace=True)\n",
    "                results['exposure_duration'] = EXPOSURE_DURATION\n",
    "                \n",
    "                for column in results.columns:\n",
    "                    results.rename(columns={column: f'{MODEL_VERSION}_{PREDICTION_ENDPOINT}_{PREDICTION_EFFECT} {column}'}, inplace=True)\n",
    "                \n",
    "                combined_results[f'{MODEL_VERSION}_{PREDICTION_ENDPOINT}_{PREDICTION_EFFECT} predictions log10(mg/L)'] = results[f'{MODEL_VERSION}_{PREDICTION_ENDPOINT}_{PREDICTION_EFFECT} predictions log10(mg/L)'].tolist()\n",
    "                combined_results[f'{MODEL_VERSION}_{PREDICTION_ENDPOINT}_{PREDICTION_EFFECT} exposure_duration'] = results[f'{MODEL_VERSION}_{PREDICTION_ENDPOINT}_{PREDICTION_EFFECT} exposure_duration'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combined_results.columns:\n",
    "    if 'exposure_duration' in col:\n",
    "        combined_results[col] = combined_results[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.to_pickle(root+f'data/tutorials/predictions/combined_predictions.pkl.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data matches\n",
    "Run locally, not on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.insert(1, 'C://Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials/')\n",
    "\n",
    "from figures.figure_utils.preprocess_data import Preprocess10x10Fold, GroupDataForPerformance\n",
    "from development_utils.preprocessing.Get_data_for_model import PreprocessData\n",
    "from tqdm.notebook import tqdm\n",
    "from inference_utils.pytorch_data_utils import check_training_data, check_training_data_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_excel('../data/development/Preprocessed_complete_data.xlsx', sheet_name='dataset')\n",
    "training_data['species_group'] = training_data.species_group.replace('crustaceans', 'invertebrates')\n",
    "all_preds = pd.read_pickle(f'../data/tutorials/predictions/combined_predictions.pkl.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "147453    False\n",
       "147454    False\n",
       "147455    False\n",
       "147456    False\n",
       "147457    False\n",
       "Name: species_group, Length: 147458, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.species_group.isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fish', 'invertebrates', 'algae'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.species_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076cfce3b9e2476d9fae2efb81d53172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fff2807a0714886bea7c8bcfb0ec3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a151640fff4556a16285e3c48d510c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['endpoint match'] = endpoint_matches\n",
      "C:\\/Users/Styrbjörn Käll/Documents/Chalmers/TRIDENT/tutorials\\inference_utils\\pytorch_data_utils.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['effect match'] = effect_matches\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for species in tqdm(species_groups):\n",
    "        MODELTYPE = f'{model}_{species}'\n",
    "        \n",
    "        for endpoint in endpointordering[MODELTYPE]:\n",
    "            for effect in effectordering[MODELTYPE]:\n",
    "                if species == 'invertebrates':\n",
    "                    all_preds = check_training_data_from_scratch(all_preds, model, 'crustaceans', endpoint, effect, path_to_data = '../data/Preprocessed_complete_data.xlsx')\n",
    "                else:\n",
    "                    all_preds = check_training_data_from_scratch(all_preds, model, species, endpoint, effect, path_to_data = '../data/Preprocessed_complete_data.xlsx')\n",
    "\n",
    "                for col in all_preds.columns:\n",
    "                    if ((col == 'species match') | (col == 'endpoint match') | (col == 'effect match')):\n",
    "                        all_preds.rename(columns={col: f'{model}_{species}_{endpoint}_{effect} {col}'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e0ff3db07241a78b0eefa88cc58ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC50EC10_fish EC50 MOR \n",
      "\n",
      "EC50EC10_fish_EC50_MOR endpoint match    3542\n",
      "EC50EC10_fish_EC50_MOR endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC50 DVP \n",
      "\n",
      "EC50EC10_fish_EC50_DVP endpoint match    3542\n",
      "EC50EC10_fish_EC50_DVP endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC50 ITX \n",
      "\n",
      "EC50EC10_fish_EC50_ITX endpoint match    3542\n",
      "EC50EC10_fish_EC50_ITX endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC50 REP \n",
      "\n",
      "EC50EC10_fish_EC50_REP endpoint match    3542\n",
      "EC50EC10_fish_EC50_REP endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC50 MPH \n",
      "\n",
      "EC50EC10_fish_EC50_MPH endpoint match    3542\n",
      "EC50EC10_fish_EC50_MPH endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC50 POP \n",
      "\n",
      "EC50EC10_fish_EC50_POP endpoint match    3542\n",
      "EC50EC10_fish_EC50_POP endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC50 GRO \n",
      "\n",
      "EC50EC10_fish_EC50_GRO endpoint match    3542\n",
      "EC50EC10_fish_EC50_GRO endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 MOR \n",
      "\n",
      "EC50EC10_fish_EC10_MOR endpoint match    2321\n",
      "EC50EC10_fish_EC10_MOR endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 DVP \n",
      "\n",
      "EC50EC10_fish_EC10_DVP endpoint match    2321\n",
      "EC50EC10_fish_EC10_DVP endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 ITX \n",
      "\n",
      "EC50EC10_fish_EC10_ITX endpoint match    2321\n",
      "EC50EC10_fish_EC10_ITX endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 REP \n",
      "\n",
      "EC50EC10_fish_EC10_REP endpoint match    2321\n",
      "EC50EC10_fish_EC10_REP endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 MPH \n",
      "\n",
      "EC50EC10_fish_EC10_MPH endpoint match    2321\n",
      "EC50EC10_fish_EC10_MPH endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 POP \n",
      "\n",
      "EC50EC10_fish_EC10_POP endpoint match    2321\n",
      "EC50EC10_fish_EC10_POP endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_fish EC10 GRO \n",
      "\n",
      "EC50EC10_fish_EC10_GRO endpoint match    2321\n",
      "EC50EC10_fish_EC10_GRO endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50_fish EC50 MOR \n",
      "\n",
      "EC50_fish_EC50_MOR endpoint match    3542\n",
      "EC50_fish_EC50_MOR endpoint match    3542\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 MOR \n",
      "\n",
      "EC10_fish_EC10_MOR endpoint match    2321\n",
      "EC10_fish_EC10_MOR endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 DVP \n",
      "\n",
      "EC10_fish_EC10_DVP endpoint match    2321\n",
      "EC10_fish_EC10_DVP endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 ITX \n",
      "\n",
      "EC10_fish_EC10_ITX endpoint match    2321\n",
      "EC10_fish_EC10_ITX endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 REP \n",
      "\n",
      "EC10_fish_EC10_REP endpoint match    2321\n",
      "EC10_fish_EC10_REP endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 MPH \n",
      "\n",
      "EC10_fish_EC10_MPH endpoint match    2321\n",
      "EC10_fish_EC10_MPH endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 POP \n",
      "\n",
      "EC10_fish_EC10_POP endpoint match    2321\n",
      "EC10_fish_EC10_POP endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC10_fish EC10 GRO \n",
      "\n",
      "EC10_fish_EC10_GRO endpoint match    2321\n",
      "EC10_fish_EC10_GRO endpoint match    2321\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_algae EC50 POP \n",
      "\n",
      "EC50EC10_algae_EC50_POP endpoint match    2843\n",
      "EC50EC10_algae_EC50_POP endpoint match    2843\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_algae EC10 POP \n",
      "\n",
      "EC50EC10_algae_EC10_POP endpoint match    2756\n",
      "EC50EC10_algae_EC10_POP endpoint match    2756\n",
      "dtype: int64 \n",
      "\n",
      "EC50_algae EC50 POP \n",
      "\n",
      "EC50_algae_EC50_POP endpoint match    2843\n",
      "EC50_algae_EC50_POP endpoint match    2843\n",
      "dtype: int64 \n",
      "\n",
      "EC10_algae EC10 POP \n",
      "\n",
      "EC10_algae_EC10_POP endpoint match    2756\n",
      "EC10_algae_EC10_POP endpoint match    2756\n",
      "dtype: int64 \n",
      "\n",
      "EC50EC10_invertebrates EC50 MOR \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50EC10_invertebrates EC50 DVP \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50EC10_invertebrates EC50 ITX \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50EC10_invertebrates EC50 REP \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50EC10_invertebrates EC50 MPH \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50EC10_invertebrates EC50 POP \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50EC10_invertebrates EC10 MOR \n",
      "\n",
      "2647 \n",
      "\n",
      "EC50EC10_invertebrates EC10 DVP \n",
      "\n",
      "2647 \n",
      "\n",
      "EC50EC10_invertebrates EC10 ITX \n",
      "\n",
      "2647 \n",
      "\n",
      "EC50EC10_invertebrates EC10 REP \n",
      "\n",
      "2647 \n",
      "\n",
      "EC50EC10_invertebrates EC10 MPH \n",
      "\n",
      "2647 \n",
      "\n",
      "EC50EC10_invertebrates EC10 POP \n",
      "\n",
      "2647 \n",
      "\n",
      "EC50_invertebrates EC50 MOR \n",
      "\n",
      "3741 \n",
      "\n",
      "EC50_invertebrates EC50 ITX \n",
      "\n",
      "3741 \n",
      "\n",
      "EC10_invertebrates EC10 MOR \n",
      "\n",
      "2647 \n",
      "\n",
      "EC10_invertebrates EC10 DVP \n",
      "\n",
      "2647 \n",
      "\n",
      "EC10_invertebrates EC10 ITX \n",
      "\n",
      "2647 \n",
      "\n",
      "EC10_invertebrates EC10 REP \n",
      "\n",
      "2647 \n",
      "\n",
      "EC10_invertebrates EC10 MPH \n",
      "\n",
      "2647 \n",
      "\n",
      "EC10_invertebrates EC10 POP \n",
      "\n",
      "2647 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SMILES_COLUMN_NAME = 'SMILES_Canonical_RDKit'\n",
    "\n",
    "for SPECIES in tqdm(species_groups):\n",
    "    \n",
    "    for model in models:\n",
    "        MODEL_VERSION = f'{model}_{SPECIES}'\n",
    "        for PREDICTION_ENDPOINT in endpointordering[MODEL_VERSION]:\n",
    "            for PREDICTION_EFFECT in effectordering[MODEL_VERSION]:\n",
    "                print(MODEL_VERSION, PREDICTION_ENDPOINT, PREDICTION_EFFECT, '\\n')\n",
    "                \n",
    "                print(all_preds[f'{MODEL_VERSION}_{PREDICTION_ENDPOINT}_{PREDICTION_EFFECT} endpoint match'].sum(), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES_Canonical_RDKit</th>\n",
       "      <th>EC50EC10_fish_EC50_MOR predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_MOR exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_DVP predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_DVP exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_ITX predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_ITX exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_REP predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_REP exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_MPH predictions log10(mg/L)</th>\n",
       "      <th>...</th>\n",
       "      <th>EC10_invertebrates_EC10_DVP endpoint match</th>\n",
       "      <th>EC10_invertebrates_EC10_DVP effect match</th>\n",
       "      <th>EC10_invertebrates_EC10_ITX endpoint match</th>\n",
       "      <th>EC10_invertebrates_EC10_ITX effect match</th>\n",
       "      <th>EC10_invertebrates_EC10_REP endpoint match</th>\n",
       "      <th>EC10_invertebrates_EC10_REP effect match</th>\n",
       "      <th>EC10_invertebrates_EC10_MPH endpoint match</th>\n",
       "      <th>EC10_invertebrates_EC10_MPH effect match</th>\n",
       "      <th>EC10_invertebrates_EC10_POP endpoint match</th>\n",
       "      <th>EC10_invertebrates_EC10_POP effect match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=[N+]([O-])c1ccc(Cl)cc1</td>\n",
       "      <td>1.172338</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.058945</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.979804</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.817641</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.761875</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nc1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>1.860993</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.756102</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.623828</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.559996</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.465644</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>O=[N+]([O-])c1ccc(O)cc1</td>\n",
       "      <td>1.350671</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.287464</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.167368</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.168845</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.143638</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>CN(C)c1ccc(C=O)cc1</td>\n",
       "      <td>1.750033</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.598159</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.482621</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.293138</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.157702</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>O=[N+]([O-])c1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>-0.168318</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.202566</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.185144</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.237345</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.251191</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147453</th>\n",
       "      <td>CCC(C(=O)O)c1ccc(N2C(=O)c3ccccc3C2=O)cc1</td>\n",
       "      <td>0.696419</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.451064</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.008681</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.244265</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147454</th>\n",
       "      <td>NC(=O)NC1NC(=O)NC1=O</td>\n",
       "      <td>2.094811</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.053785</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.882532</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.951870</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.902873</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147455</th>\n",
       "      <td>S=C(SSSSSSC(=S)N1CCCCC1)N1CCCCC1</td>\n",
       "      <td>0.741298</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.601649</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.545263</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.294324</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.128343</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147456</th>\n",
       "      <td>CC1CCC(C(C)C)CC1</td>\n",
       "      <td>0.768781</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.717363</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.643591</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.646532</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.607147</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147457</th>\n",
       "      <td>COc1cc(OC)nc(NC(=O)NS(=O)(=O)Cc2ccccc2C(=O)O)n1</td>\n",
       "      <td>2.018202</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.949705</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.795435</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.776394</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6505 rows × 277 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 SMILES_Canonical_RDKit  \\\n",
       "0                              O=[N+]([O-])c1ccc(Cl)cc1   \n",
       "34                              Nc1ccc([N+](=O)[O-])cc1   \n",
       "63                              O=[N+]([O-])c1ccc(O)cc1   \n",
       "403                                  CN(C)c1ccc(C=O)cc1   \n",
       "404                  O=[N+]([O-])c1ccc([N+](=O)[O-])cc1   \n",
       "...                                                 ...   \n",
       "147453         CCC(C(=O)O)c1ccc(N2C(=O)c3ccccc3C2=O)cc1   \n",
       "147454                             NC(=O)NC1NC(=O)NC1=O   \n",
       "147455                 S=C(SSSSSSC(=S)N1CCCCC1)N1CCCCC1   \n",
       "147456                                 CC1CCC(C(C)C)CC1   \n",
       "147457  COc1cc(OC)nc(NC(=O)NS(=O)(=O)Cc2ccccc2C(=O)O)n1   \n",
       "\n",
       "        EC50EC10_fish_EC50_MOR predictions log10(mg/L)  \\\n",
       "0                                             1.172338   \n",
       "34                                            1.860993   \n",
       "63                                            1.350671   \n",
       "403                                           1.750033   \n",
       "404                                          -0.168318   \n",
       "...                                                ...   \n",
       "147453                                        0.696419   \n",
       "147454                                        2.094811   \n",
       "147455                                        0.741298   \n",
       "147456                                        0.768781   \n",
       "147457                                        2.018202   \n",
       "\n",
       "        EC50EC10_fish_EC50_MOR exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_DVP predictions log10(mg/L)  \\\n",
       "0                                             1.058945   \n",
       "34                                            1.756102   \n",
       "63                                            1.287464   \n",
       "403                                           1.598159   \n",
       "404                                          -0.202566   \n",
       "...                                                ...   \n",
       "147453                                        0.495500   \n",
       "147454                                        2.053785   \n",
       "147455                                        0.601649   \n",
       "147456                                        0.717363   \n",
       "147457                                        1.949705   \n",
       "\n",
       "        EC50EC10_fish_EC50_DVP exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_ITX predictions log10(mg/L)  \\\n",
       "0                                             0.979804   \n",
       "34                                            1.623828   \n",
       "63                                            1.167368   \n",
       "403                                           1.482621   \n",
       "404                                          -0.185144   \n",
       "...                                                ...   \n",
       "147453                                        0.451064   \n",
       "147454                                        1.882532   \n",
       "147455                                        0.545263   \n",
       "147456                                        0.643591   \n",
       "147457                                        1.795435   \n",
       "\n",
       "        EC50EC10_fish_EC50_ITX exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_REP predictions log10(mg/L)  \\\n",
       "0                                             0.817641   \n",
       "34                                            1.559996   \n",
       "63                                            1.168845   \n",
       "403                                           1.293138   \n",
       "404                                          -0.237345   \n",
       "...                                                ...   \n",
       "147453                                       -0.008681   \n",
       "147454                                        1.951870   \n",
       "147455                                        0.294324   \n",
       "147456                                        0.646532   \n",
       "147457                                        1.820700   \n",
       "\n",
       "        EC50EC10_fish_EC50_REP exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_MPH predictions log10(mg/L)  ...  \\\n",
       "0                                             0.761875  ...   \n",
       "34                                            1.465644  ...   \n",
       "63                                            1.143638  ...   \n",
       "403                                           1.157702  ...   \n",
       "404                                          -0.251191  ...   \n",
       "...                                                ...  ...   \n",
       "147453                                       -0.244265  ...   \n",
       "147454                                        1.902873  ...   \n",
       "147455                                        0.128343  ...   \n",
       "147456                                        0.607147  ...   \n",
       "147457                                        1.776394  ...   \n",
       "\n",
       "        EC10_invertebrates_EC10_DVP endpoint match  \\\n",
       "0                                                1   \n",
       "34                                               1   \n",
       "63                                               1   \n",
       "403                                              0   \n",
       "404                                              0   \n",
       "...                                            ...   \n",
       "147453                                           0   \n",
       "147454                                           0   \n",
       "147455                                           0   \n",
       "147456                                           0   \n",
       "147457                                           0   \n",
       "\n",
       "        EC10_invertebrates_EC10_DVP effect match  \\\n",
       "0                                              0   \n",
       "34                                             0   \n",
       "63                                             0   \n",
       "403                                            0   \n",
       "404                                            0   \n",
       "...                                          ...   \n",
       "147453                                         0   \n",
       "147454                                         0   \n",
       "147455                                         0   \n",
       "147456                                         0   \n",
       "147457                                         0   \n",
       "\n",
       "        EC10_invertebrates_EC10_ITX endpoint match  \\\n",
       "0                                                1   \n",
       "34                                               1   \n",
       "63                                               1   \n",
       "403                                              0   \n",
       "404                                              0   \n",
       "...                                            ...   \n",
       "147453                                           0   \n",
       "147454                                           0   \n",
       "147455                                           0   \n",
       "147456                                           0   \n",
       "147457                                           0   \n",
       "\n",
       "        EC10_invertebrates_EC10_ITX effect match  \\\n",
       "0                                              1   \n",
       "34                                             1   \n",
       "63                                             1   \n",
       "403                                            0   \n",
       "404                                            0   \n",
       "...                                          ...   \n",
       "147453                                         0   \n",
       "147454                                         0   \n",
       "147455                                         0   \n",
       "147456                                         0   \n",
       "147457                                         0   \n",
       "\n",
       "        EC10_invertebrates_EC10_REP endpoint match  \\\n",
       "0                                                1   \n",
       "34                                               1   \n",
       "63                                               1   \n",
       "403                                              0   \n",
       "404                                              0   \n",
       "...                                            ...   \n",
       "147453                                           0   \n",
       "147454                                           0   \n",
       "147455                                           0   \n",
       "147456                                           0   \n",
       "147457                                           0   \n",
       "\n",
       "        EC10_invertebrates_EC10_REP effect match  \\\n",
       "0                                              1   \n",
       "34                                             0   \n",
       "63                                             1   \n",
       "403                                            0   \n",
       "404                                            0   \n",
       "...                                          ...   \n",
       "147453                                         0   \n",
       "147454                                         0   \n",
       "147455                                         0   \n",
       "147456                                         0   \n",
       "147457                                         0   \n",
       "\n",
       "        EC10_invertebrates_EC10_MPH endpoint match  \\\n",
       "0                                                1   \n",
       "34                                               1   \n",
       "63                                               1   \n",
       "403                                              0   \n",
       "404                                              0   \n",
       "...                                            ...   \n",
       "147453                                           0   \n",
       "147454                                           0   \n",
       "147455                                           0   \n",
       "147456                                           0   \n",
       "147457                                           0   \n",
       "\n",
       "        EC10_invertebrates_EC10_MPH effect match  \\\n",
       "0                                              0   \n",
       "34                                             0   \n",
       "63                                             0   \n",
       "403                                            0   \n",
       "404                                            0   \n",
       "...                                          ...   \n",
       "147453                                         0   \n",
       "147454                                         0   \n",
       "147455                                         0   \n",
       "147456                                         0   \n",
       "147457                                         0   \n",
       "\n",
       "        EC10_invertebrates_EC10_POP endpoint match  \\\n",
       "0                                                1   \n",
       "34                                               1   \n",
       "63                                               1   \n",
       "403                                              0   \n",
       "404                                              0   \n",
       "...                                            ...   \n",
       "147453                                           0   \n",
       "147454                                           0   \n",
       "147455                                           0   \n",
       "147456                                           0   \n",
       "147457                                           0   \n",
       "\n",
       "        EC10_invertebrates_EC10_POP effect match  \n",
       "0                                              0  \n",
       "34                                             0  \n",
       "63                                             0  \n",
       "403                                            0  \n",
       "404                                            0  \n",
       "...                                          ...  \n",
       "147453                                         0  \n",
       "147454                                         0  \n",
       "147455                                         0  \n",
       "147456                                         0  \n",
       "147457                                         0  \n",
       "\n",
       "[6505 rows x 277 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.to_pickle(f'../data/tutorials/predictions/combined_predictions_and_training_data.pkl.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add errors from 10x10 CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from figures.figure_utils.preprocess_data import Preprocess10x10Fold, GroupDataForPerformance\n",
    "from development_utils.preprocessing.Get_data_for_model import PreprocessData\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data = pd.read_pickle('../data/tutorials/predictions/combined_predictions_and_training_data.pkl.zip', compression='zip')\n",
    "training_data['SMILES'] = training_data['SMILES_Canonical_RDKit'].copy()\n",
    "training_data = PreprocessData(training_data).GetCanonicalSMILES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(x, error_dict):\n",
    "    try:\n",
    "        return error_dict[x]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a027fc16ac345559e11715454c6e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab608b52b2b1479ca1538954e5e6b63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99abb7b44daa43ff99ca736126555116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for species_group in ['fish', 'invertebrates','algae']:\n",
    "    for model in tqdm(['EC50','EC10','EC50EC10']):\n",
    "        if model != 'EC50EC10':\n",
    "            cvpreds = Preprocess10x10Fold(name=f'{model}_{species_group}', uselogdata=True, full_filepath=f'../data/results/{model}_{species_group}_predictions_100x_CV_RDkit.pkl.zip')\n",
    "        else:\n",
    "            cvpreds = Preprocess10x10Fold(name=f'{model}_{species_group}', uselogdata=True, full_filepath=f'../data/results/{model}_{species_group}_withoverlap_predictions_100x_CV_RDkit.pkl.zip')\n",
    "        \n",
    "        wavgcv = GroupDataForPerformance(cvpreds)\n",
    "        wavgcv['SMILES'] = wavgcv['Canonical_SMILES_figures'].copy()\n",
    "        wavgcv = PreprocessData(wavgcv).GetCanonicalSMILES()\n",
    "        error_dict = dict(zip(wavgcv.SMILES_Canonical_RDKit.tolist(), wavgcv.L1error.tolist()))\n",
    "\n",
    "        training_data[f'{model}_{species_group} L1Error'] = training_data.SMILES_Canonical_RDKit.apply(lambda x: match(x, error_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_pickle('../data/tutorials/predictions/combined_predictions_and_errors.pkl.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES_Canonical_RDKit</th>\n",
       "      <th>EC50EC10_fish_EC50_MOR predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_MOR exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_DVP predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_DVP exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_ITX predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_ITX exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_REP predictions log10(mg/L)</th>\n",
       "      <th>EC50EC10_fish_EC50_REP exposure_duration</th>\n",
       "      <th>EC50EC10_fish_EC50_MPH predictions log10(mg/L)</th>\n",
       "      <th>...</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>EC50_fish L1Error</th>\n",
       "      <th>EC10_fish L1Error</th>\n",
       "      <th>EC50EC10_fish L1Error</th>\n",
       "      <th>EC50_invertebrates L1Error</th>\n",
       "      <th>EC10_invertebrates L1Error</th>\n",
       "      <th>EC50EC10_invertebrates L1Error</th>\n",
       "      <th>EC50_algae L1Error</th>\n",
       "      <th>EC10_algae L1Error</th>\n",
       "      <th>EC50EC10_algae L1Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=[N+]([O-])c1ccc(Cl)cc1</td>\n",
       "      <td>1.172338</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.058945</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.979804</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.817641</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.761875</td>\n",
       "      <td>...</td>\n",
       "      <td>O=[N+]([O-])c1ccc(Cl)cc1</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>0.373479</td>\n",
       "      <td>0.125743</td>\n",
       "      <td>0.065393</td>\n",
       "      <td>0.402708</td>\n",
       "      <td>0.304046</td>\n",
       "      <td>0.698894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nc1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>1.860993</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.756102</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.623828</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.559996</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.465644</td>\n",
       "      <td>...</td>\n",
       "      <td>Nc1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>0.241841</td>\n",
       "      <td>0.292932</td>\n",
       "      <td>0.161388</td>\n",
       "      <td>0.800447</td>\n",
       "      <td>0.360655</td>\n",
       "      <td>0.427892</td>\n",
       "      <td>0.575160</td>\n",
       "      <td>0.567851</td>\n",
       "      <td>0.544045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>O=[N+]([O-])c1ccc(O)cc1</td>\n",
       "      <td>1.350671</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.287464</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.167368</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.168845</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.143638</td>\n",
       "      <td>...</td>\n",
       "      <td>O=[N+]([O-])c1ccc(O)cc1</td>\n",
       "      <td>0.436332</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>0.431412</td>\n",
       "      <td>0.184236</td>\n",
       "      <td>0.304895</td>\n",
       "      <td>0.271843</td>\n",
       "      <td>0.560873</td>\n",
       "      <td>0.522284</td>\n",
       "      <td>0.759680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>CN(C)c1ccc(C=O)cc1</td>\n",
       "      <td>1.750033</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.598159</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.482621</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.293138</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.157702</td>\n",
       "      <td>...</td>\n",
       "      <td>CN(C)c1ccc(C=O)cc1</td>\n",
       "      <td>0.245164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>O=[N+]([O-])c1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>-0.168318</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.202566</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.185144</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.237345</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.251191</td>\n",
       "      <td>...</td>\n",
       "      <td>O=[N+]([O-])c1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>1.076959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.117791</td>\n",
       "      <td>3.271399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.397170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147453</th>\n",
       "      <td>CCC(C(=O)O)c1ccc(N2C(=O)c3ccccc3C2=O)cc1</td>\n",
       "      <td>0.696419</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.451064</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.008681</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-0.244265</td>\n",
       "      <td>...</td>\n",
       "      <td>CCC(C(=O)O)c1ccc(N2C(=O)c3ccccc3C2=O)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785128</td>\n",
       "      <td>1.308006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147454</th>\n",
       "      <td>NC(=O)NC1NC(=O)NC1=O</td>\n",
       "      <td>2.094811</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.053785</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.882532</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.951870</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.902873</td>\n",
       "      <td>...</td>\n",
       "      <td>NC(=O)NC1NC(=O)NC1=O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114482</td>\n",
       "      <td>0.316235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147455</th>\n",
       "      <td>S=C(SSSSSSC(=S)N1CCCCC1)N1CCCCC1</td>\n",
       "      <td>0.741298</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.601649</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.545263</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.294324</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.128343</td>\n",
       "      <td>...</td>\n",
       "      <td>S=C(SSSSSSC(=S)N1CCCCC1)N1CCCCC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.185539</td>\n",
       "      <td>2.263958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147456</th>\n",
       "      <td>CC1CCC(C(C)C)CC1</td>\n",
       "      <td>0.768781</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.717363</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.643591</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.646532</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.607147</td>\n",
       "      <td>...</td>\n",
       "      <td>CC1CCC(C(C)C)CC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179110</td>\n",
       "      <td>0.241676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147457</th>\n",
       "      <td>COc1cc(OC)nc(NC(=O)NS(=O)(=O)Cc2ccccc2C(=O)O)n1</td>\n",
       "      <td>2.018202</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.949705</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.795435</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.820700</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.776394</td>\n",
       "      <td>...</td>\n",
       "      <td>COc1cc(OC)nc(NC(=O)NS(=O)(=O)Cc2ccccc2C(=O)O)n1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.048782</td>\n",
       "      <td>2.045363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6505 rows × 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 SMILES_Canonical_RDKit  \\\n",
       "0                              O=[N+]([O-])c1ccc(Cl)cc1   \n",
       "34                              Nc1ccc([N+](=O)[O-])cc1   \n",
       "63                              O=[N+]([O-])c1ccc(O)cc1   \n",
       "403                                  CN(C)c1ccc(C=O)cc1   \n",
       "404                  O=[N+]([O-])c1ccc([N+](=O)[O-])cc1   \n",
       "...                                                 ...   \n",
       "147453         CCC(C(=O)O)c1ccc(N2C(=O)c3ccccc3C2=O)cc1   \n",
       "147454                             NC(=O)NC1NC(=O)NC1=O   \n",
       "147455                 S=C(SSSSSSC(=S)N1CCCCC1)N1CCCCC1   \n",
       "147456                                 CC1CCC(C(C)C)CC1   \n",
       "147457  COc1cc(OC)nc(NC(=O)NS(=O)(=O)Cc2ccccc2C(=O)O)n1   \n",
       "\n",
       "        EC50EC10_fish_EC50_MOR predictions log10(mg/L)  \\\n",
       "0                                             1.172338   \n",
       "34                                            1.860993   \n",
       "63                                            1.350671   \n",
       "403                                           1.750033   \n",
       "404                                          -0.168318   \n",
       "...                                                ...   \n",
       "147453                                        0.696419   \n",
       "147454                                        2.094811   \n",
       "147455                                        0.741298   \n",
       "147456                                        0.768781   \n",
       "147457                                        2.018202   \n",
       "\n",
       "        EC50EC10_fish_EC50_MOR exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_DVP predictions log10(mg/L)  \\\n",
       "0                                             1.058945   \n",
       "34                                            1.756102   \n",
       "63                                            1.287464   \n",
       "403                                           1.598159   \n",
       "404                                          -0.202566   \n",
       "...                                                ...   \n",
       "147453                                        0.495500   \n",
       "147454                                        2.053785   \n",
       "147455                                        0.601649   \n",
       "147456                                        0.717363   \n",
       "147457                                        1.949705   \n",
       "\n",
       "        EC50EC10_fish_EC50_DVP exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_ITX predictions log10(mg/L)  \\\n",
       "0                                             0.979804   \n",
       "34                                            1.623828   \n",
       "63                                            1.167368   \n",
       "403                                           1.482621   \n",
       "404                                          -0.185144   \n",
       "...                                                ...   \n",
       "147453                                        0.451064   \n",
       "147454                                        1.882532   \n",
       "147455                                        0.545263   \n",
       "147456                                        0.643591   \n",
       "147457                                        1.795435   \n",
       "\n",
       "        EC50EC10_fish_EC50_ITX exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_REP predictions log10(mg/L)  \\\n",
       "0                                             0.817641   \n",
       "34                                            1.559996   \n",
       "63                                            1.168845   \n",
       "403                                           1.293138   \n",
       "404                                          -0.237345   \n",
       "...                                                ...   \n",
       "147453                                       -0.008681   \n",
       "147454                                        1.951870   \n",
       "147455                                        0.294324   \n",
       "147456                                        0.646532   \n",
       "147457                                        1.820700   \n",
       "\n",
       "        EC50EC10_fish_EC50_REP exposure_duration  \\\n",
       "0                                           96.0   \n",
       "34                                          96.0   \n",
       "63                                          96.0   \n",
       "403                                         96.0   \n",
       "404                                         96.0   \n",
       "...                                          ...   \n",
       "147453                                      96.0   \n",
       "147454                                      96.0   \n",
       "147455                                      96.0   \n",
       "147456                                      96.0   \n",
       "147457                                      96.0   \n",
       "\n",
       "        EC50EC10_fish_EC50_MPH predictions log10(mg/L)  ...  \\\n",
       "0                                             0.761875  ...   \n",
       "34                                            1.465644  ...   \n",
       "63                                            1.143638  ...   \n",
       "403                                           1.157702  ...   \n",
       "404                                          -0.251191  ...   \n",
       "...                                                ...  ...   \n",
       "147453                                       -0.244265  ...   \n",
       "147454                                        1.902873  ...   \n",
       "147455                                        0.128343  ...   \n",
       "147456                                        0.607147  ...   \n",
       "147457                                        1.776394  ...   \n",
       "\n",
       "                                                 SMILES  EC50_fish L1Error  \\\n",
       "0                              O=[N+]([O-])c1ccc(Cl)cc1           0.010166   \n",
       "34                              Nc1ccc([N+](=O)[O-])cc1           0.241841   \n",
       "63                              O=[N+]([O-])c1ccc(O)cc1           0.436332   \n",
       "403                                  CN(C)c1ccc(C=O)cc1           0.245164   \n",
       "404                  O=[N+]([O-])c1ccc([N+](=O)[O-])cc1           1.076959   \n",
       "...                                                 ...                ...   \n",
       "147453         CCC(C(=O)O)c1ccc(N2C(=O)c3ccccc3C2=O)cc1                NaN   \n",
       "147454                             NC(=O)NC1NC(=O)NC1=O                NaN   \n",
       "147455                 S=C(SSSSSSC(=S)N1CCCCC1)N1CCCCC1                NaN   \n",
       "147456                                 CC1CCC(C(C)C)CC1                NaN   \n",
       "147457  COc1cc(OC)nc(NC(=O)NS(=O)(=O)Cc2ccccc2C(=O)O)n1                NaN   \n",
       "\n",
       "        EC10_fish L1Error  EC50EC10_fish L1Error  EC50_invertebrates L1Error  \\\n",
       "0                0.373479               0.125743                    0.065393   \n",
       "34               0.292932               0.161388                    0.800447   \n",
       "63               0.010344               0.431412                    0.184236   \n",
       "403                   NaN               0.240691                         NaN   \n",
       "404                   NaN               1.117791                    3.271399   \n",
       "...                   ...                    ...                         ...   \n",
       "147453                NaN                    NaN                         NaN   \n",
       "147454                NaN                    NaN                         NaN   \n",
       "147455                NaN                    NaN                         NaN   \n",
       "147456                NaN                    NaN                         NaN   \n",
       "147457                NaN                    NaN                         NaN   \n",
       "\n",
       "        EC10_invertebrates L1Error  EC50EC10_invertebrates L1Error  \\\n",
       "0                         0.402708                        0.304046   \n",
       "34                        0.360655                        0.427892   \n",
       "63                        0.304895                        0.271843   \n",
       "403                            NaN                             NaN   \n",
       "404                            NaN                        3.397170   \n",
       "...                            ...                             ...   \n",
       "147453                         NaN                             NaN   \n",
       "147454                         NaN                             NaN   \n",
       "147455                         NaN                             NaN   \n",
       "147456                         NaN                             NaN   \n",
       "147457                         NaN                             NaN   \n",
       "\n",
       "        EC50_algae L1Error  EC10_algae L1Error  EC50EC10_algae L1Error  \n",
       "0                 0.698894                 NaN                0.677930  \n",
       "34                0.575160            0.567851                0.544045  \n",
       "63                0.560873            0.522284                0.759680  \n",
       "403                    NaN                 NaN                     NaN  \n",
       "404                    NaN                 NaN                     NaN  \n",
       "...                    ...                 ...                     ...  \n",
       "147453                 NaN            0.785128                1.308006  \n",
       "147454                 NaN            0.114482                0.316235  \n",
       "147455                 NaN            2.185539                2.263958  \n",
       "147456                 NaN            0.179110                0.241676  \n",
       "147457                 NaN            2.048782                2.045363  \n",
       "\n",
       "[6505 rows x 287 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors for Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_excel('../data/tutorials/predictions/Weighted_avg_error_per_SMILES.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
